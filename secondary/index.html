<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SSNJ Formant Speech Synthesizer</title>
  <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>
  <style>
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif;
      line-height: 1.6;
      color: #333;
      max-width: 900px;
      margin: 2rem auto;
      padding: 1.5rem;
      background: linear-gradient(135deg, #f5f7fa 0%, #e4edf5 100%);
    }
    h1 {
      text-align: center;
      margin-bottom: 1.5rem;
      color: #2c3e50;
      font-size: 2.2rem;
    }
    .container {
      background: white;
      border-radius: 16px;
      box-shadow: 0 10px 30px rgba(0,0,0,0.08);
      padding: 2rem;
      margin-top: 1rem;
    }
    textarea {
      width: 100%;
      height: 160px;
      padding: 1rem;
      border: 2px solid #ddd;
      border-radius: 12px;
      font-size: 1rem;
      margin-bottom: 1.2rem;
      resize: vertical;
      transition: border-color 0.3s;
      font-family: monospace;
    }
    textarea:focus {
      outline: none;
      border-color: #4a6fa5;
      box-shadow: 0 0 0 3px rgba(74, 111, 165, 0.15);
    }
    .mode-indicator {
      text-align: center;
      margin-bottom: 1rem;
      padding: 0.6rem;
      border-radius: 10px;
      font-weight: 600;
      transition: all 0.3s ease;
    }
    .mode-word { background: #e8f5e9; color: #2e7d32; }
    .mode-phoneme { background: #e3f2fd; color: #1565c0; }
    .mode-spec { background: #f3e5f5; color: #4a148c; }
    .controls {
      display: flex;
      gap: 1rem;
      margin-bottom: 1.5rem;
      flex-wrap: wrap;
      align-items: stretch;
    }
    button {
      flex: 1;
      min-width: 120px;
      padding: 0.9rem 1.5rem;
      background: linear-gradient(to bottom, #4a6fa5, #2c3e50);
      color: white;
      border: none;
      border-radius: 12px;
      font-size: 1rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 4px 15px rgba(44, 62, 80, 0.3);
    }
    button:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(44, 62, 80, 0.4);
    }
    button:active:not(:disabled) {
      transform: translateY(0);
    }
    button:disabled {
      background: #bdc3c7;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    .download-btn {
      background: linear-gradient(to bottom, #27ae60, #219653);
      min-width: 140px;
    }
    .download-btn:disabled {
      background: #bdc3c7;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    .audio-container {
      margin-top: 1.5rem;
      text-align: center;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 1rem;
    }
    audio {
      width: 100%;
      max-width: 600px;
      margin-top: 0.5rem;
    }
    .audio-actions {
      display: flex;
      gap: 1rem;
      margin-top: 0.5rem;
      flex-wrap: wrap;
      justify-content: center;
    }
    .audio-actions button {
      padding: 0.7rem 1.2rem;
      font-size: 0.95rem;
      min-width: 100px;
    }
    #status {
      text-align: center;
      margin: 1.2rem 0;
      min-height: 1.8rem;
      font-weight: 500;
      padding: 0.5rem;
      border-radius: 8px;
    }
    .status-loading { color: #2980b9; background: #e3f2fd; }
    .status-ready { color: #27ae60; background: #e8f5e9; }
    .status-error { color: #e74c3c; background: #fadbd8; }
    .status-processing { color: #8e44ad; background: #f5f9fc; }
    .info-box {
      background: #e3f2fd;
      border-left: 4px solid #2980b9;
      padding: 1rem;
      border-radius: 0 8px 8px 0;
      margin: 1.5rem 0;
      font-size: 0.95rem;
    }
    .phoneme-guide, .spec-guide {
      padding: 1rem;
      border-radius: 0 8px 8px 0;
      margin: 1rem 0;
      font-family: monospace;
      font-size: 0.9rem;
    }
    .phoneme-guide {
      background: #e8f5e9;
      border-left: 4px solid #2e7d32;
    }
    .spec-guide {
      background: #f3e5f5;
      border-left: 4px solid #4a148c;
    }
    .limitations {
      font-size: 0.9rem;
      color: #7f8c8d;
      margin-top: 0.5rem;
      padding-top: 0.5rem;
      border-top: 1px dashed #bdc3c7;
    }
    .tabs {
      display: flex;
      margin-bottom: 1rem;
      gap: 0.5rem;
      flex-wrap: wrap;
    }
    .tab {
      padding: 0.6rem 1rem;
      background: #e0e7ff;
      border: none;
      border-radius: 8px 8px 0 0;
      cursor: pointer;
      font-weight: 600;
      flex: 1;
      min-width: 120px;
      text-align: center;
    }
    .tab.active {
      background: #4a6fa5;
      color: white;
    }
    .tab-content {
      display: none;
    }
    .tab-content.active {
      display: block;
    }
    .example-btn {
      background: linear-gradient(to bottom, #9c27b0, #6a1b9a);
    }
    @media (max-width: 600px) {
      .controls { flex-direction: column; }
      button { width: 100%; min-width: auto; }
      body { padding: 1rem; margin: 1rem; }
      .tabs { flex-direction: column; }
      .tab { border-radius: 8px; width: 100%; }
      .audio-actions { flex-direction: column; width: 100%; }
    }
  </style>
</head>
<body>
  <h1>ü§ñ Jim Formant Speech Synthesizer</h1>
  <div class="container">
    <div class="tabs">
      <button class="tab active" data-tab="word">üî§ Word Mode</button>
      <button class="tab" data-tab="phoneme">üëÑ Phoneme Mode</button>
      <button class="tab" data-tab="spec">üéõÔ∏è Spec Mode</button>
    </div>
    <div id="word-tab" class="tab-content active">
      <textarea id="textInput" placeholder="Enter words (e.g., 'hello world robot')">hello world this is a robot voice</textarea>
      <div class="mode-indicator mode-word">üî§ Strict Word Mode (no auto-detection)</div>
    </div>
    <div id="phoneme-tab" class="tab-content">
      <textarea id="phonemeInput" placeholder="Enter phonemes separated by spaces (e.g., HH EH L OW SIL W ER L D)">HH EH L OW SIL W ER L D</textarea>
      <div class="phoneme-guide">
        <strong>Valid Phonemes (ARPABET):</strong><br>
        SIL, AH, AE, AA, AO, EH, EY, IH, IY, OW, UH, UW, ER,<br>
        B, D, G, P, T, K, M, N, NG, L, R, F, S, SH, TH, DH, V, Z, ZH, W, Y, HH, CH, JH<br>
        <strong>Tip:</strong> Use SIL between words for natural pauses
      </div>
    </div>
    <div id="spec-tab" class="tab-content">
      <textarea id="specInput" placeholder="Enter spec format: PHON DUR RISE FALL"># Hello World with pitch variation
HH       0.150  0.010  120.0
EH       0.140  0.018  115.0
L        0.120  0.008  115.0
OW       0.180  0.018  115.0 105.0
SIL      0.120  0.008  0.0
W        0.120  0.008  110.0
ER       0.140  0.018  115.0
L        0.120  0.008  115.0
D        0.068  0.008  0.0</textarea>
      <div class="spec-guide">
        <strong>Spec Format:</strong> PHON DURATION RISE [FALL]<br>
        Example: <code>OW 0.180 0.018 115.0 105.0</code><br>
        ‚Ä¢ DURATION in seconds<br>
        ‚Ä¢ RISE = starting pitch (Hz)<br>
        ‚Ä¢ FALL (optional) = ending pitch (Hz)
        <strong>Valid Phonemes (ARPABET):</strong><br>
        SIL, AH, AE, AA, AO, EH, EY, IH, IY, OW, UH, UW, ER,<br>
        B, D, G, P, T, K, M, N, NG, L, R, F, S, SH, TH, DH, V, Z, ZH, W, Y, HH, CH, JH<br>
        <strong>Tip:</strong> Use SIL between words for natural pauses
      </div>
    </div>
    <div class="controls">
      <button id="synthesizeBtn" disabled>‚ñ∂Ô∏è Synthesize</button>
      <button id="playExampleBtn" disabled class="example-btn">üéµ Example</button>
      <button id="downloadBtn" disabled class="download-btn">üíæ Download WAV</button>
    </div>
    <div id="status" class="status-loading">Initializing Pyodide environment...</div>
    <div class="info-box">
      <strong>‚ÑπÔ∏è About Jim:</strong> Formant-based speech synthesizer running entirely in-browser via Pyodide.<br>
      Converts text ‚Üí phonemes ‚Üí formant synthesis ‚Üí WAV audio. No server required!
    </div>
    <div class="audio-container">
      <audio id="audioPlayer" controls>Your browser does not support the audio element.</audio>
      <div class="audio-actions">
        <button id="playBtn" disabled>‚ñ∂Ô∏è Play</button>
        <button id="pauseBtn" disabled>‚è∏Ô∏è Pause</button>
      </div>
    </div>
    <div class="limitations">
      ‚ö†Ô∏è <strong>Limitations:</strong> GitHub Pages has no server-side processing. All synthesis happens in your browser via WebAssembly (Pyodide). First load may take 15-30 seconds to initialize Python environment.
    </div>
  </div>
  <script type="text/python" id="jim-source">
import numpy as np
import scipy.signal as sig
import wave
import re
import json
from typing import Dict, List
from io import BytesIO

# ======================
# JIM.PY CORE CODE (Modified for browser execution)
# ======================
smp = 97000

BYTE_TO_PHONEME = {
    0x00: 'SIL', 0x01: 'AH', 0x02: 'AE', 0x03: 'AA', 0x04: 'AO', 0x05: 'EH', 0x06: 'EY',
    0x07: 'IH', 0x08: 'IY', 0x09: 'OW', 0x0A: 'UH', 0x0B: 'UW', 0x0C: 'ER', 0x0D: 'B',
    0x0E: 'D', 0x0F: 'G', 0x10: 'P', 0x11: 'T', 0x12: 'K', 0x13: 'M', 0x14: 'N', 0x15: 'NG',
    0x16: 'L', 0x17: 'R', 0x18: 'F', 0x19: 'S', 0x1A: 'SH', 0x1B: 'TH', 0x1C: 'DH', 0x1D: 'V',
    0x1E: 'Z', 0x1F: 'ZH', 0x20: 'W', 0x21: 'Y', 0x22: 'HH', 0x23: 'CH', 0x24: 'JH',
}

PHONEME_TO_BYTE = {v: k for k, v in BYTE_TO_PHONEME.items()}
VALID_PHONEMES = set(PHONEME_TO_BYTE.keys())
VOWELS = {'AH','AE','AA','AO','EH','EY','IH','IY','OW','UH','UW','ER'}
STOPS = {'P','T','K','B','D','G','CH'}
FRICATIVES_UNVOICED = {'F','S','SH','TH','HH'}
FRICATIVES_VOICED = {'V','Z','ZH','DH'}

class Voice:
    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self.phonemes: Dict[str, Dict] = {}
    
    def get_phoneme_data(self, phoneme: str) -> Dict:
        if phoneme.endswith('_FINAL'):
            base_ph = phoneme.replace('_FINAL', '')
            data = self.phonemes.get(base_ph, self.phonemes.get('SIL', {})).copy()
            if base_ph in VOWELS:
                data['length'] = min(data.get('length', 0.14) * 1.4, 0.35)
            return data
        return self.phonemes.get(phoneme, self.phonemes.get('SIL', {}))
    
    def save(self, filepath: str) -> None:
        pass  # Disabled for browser
    
    @classmethod
    def load(cls, filepath: str) -> 'Voice':
        pass  # Disabled for browser

class DefaultVoice(Voice):
    def __init__(self):
        super().__init__("Default", "Built-in robotic voice")
        self.phonemes = {
            'AH': {'f1': 700, 'f2': 1100, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AE': {'f1': 650, 'f2': 1250, 'f3': 2500, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AA': {'f1': 620, 'f2': 1180, 'f3': 2550, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AO': {'f1': 550, 'f2': 850, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'EH': {'f1': 530, 'f2': 1700, 'f3': 2450, 'f4': 115, 'length': 0.14, 'voiced': True},
            'EY': {'f1': 400, 'f2': 2100, 'f3': 2800, 'f4': 115, 'length': 0.14, 'voiced': True},
            'IH': {'f1': 420, 'f2': 1950, 'f3': 2500, 'f4': 115, 'length': 0.14, 'voiced': True},
            'IY': {'f1': 300, 'f2': 2250, 'f3': 3000, 'f4': 115, 'length': 0.14, 'voiced': True},
            'OW': {'f1': 450, 'f2': 900, 'f3': 2350, 'f4': 115, 'length': 0.14, 'voiced': True},
            'UH': {'f1': 400, 'f2': 650, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'UW': {'f1': 330, 'f2': 900, 'f3': 2200, 'f4': 115, 'length': 0.14, 'voiced': True},
            'ER': {'f1': 480, 'f2': 1180, 'f3': 1650, 'f4': 115, 'length': 0.14, 'voiced': True},
            'M':  {'f1': 350, 'f2': 1050, 'f3': 2250, 'f4': 115, 'length': 0.12, 'voiced': True},
            'N':  {'f1': 320, 'f2': 1150, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'NG': {'f1': 280, 'f2': 950, 'f3': 2350, 'f4': 115, 'length': 0.12, 'voiced': True},
            'L':  {'f1': 400, 'f2': 1150, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'R':  {'f1': 450, 'f2': 1250, 'f3': 1500, 'f4': 115, 'length': 0.12, 'voiced': True},
            'DH': {'f1': 380, 'f2': 1650, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'V':  {'f1': 380, 'f2': 1550, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'Z':  {'f1': 380, 'f2': 1750, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'ZH': {'f1': 380, 'f2': 1450, 'f3': 2250, 'f4': 115, 'length': 0.12, 'voiced': True},
            'W':  {'f1': 350, 'f2': 700, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'Y':  {'f1': 350, 'f2': 2050, 'f3': 2650, 'f4': 115, 'length': 0.12, 'voiced': True},
            'JH': {'f1': 400, 'f2': 1650, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'B':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'D':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'G':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'P':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'T':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'K':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'F':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'S':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'SH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'TH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'HH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'CH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'SIL': {'f1': 0, 'f2': 0, 'f3': 0, 'f4': 0, 'length': 0.19, 'voiced': 'silence'},
        }

class VoiceRegistry:
    def __init__(self):
        self.voices: Dict[str, Voice] = {'Default': DefaultVoice()}
        self.current_voice: Voice = self.voices['Default']
    
    def set_current_voice(self, name: str) -> bool:
        if name in self.voices:
            self.current_voice = self.voices[name]
            return True
        return False
    
    def list_voices(self) -> Dict[str, Voice]:
        return self.voices

VOICE_REGISTRY = VoiceRegistry()

WORD_MAP = {
    'hello': ['HH', 'EH', 'L', 'AO', 'OW'], 'world': ['W', 'ER', 'L', 'D'], 'test': ['T', 'EH', 'S', 'T'],
    'one': ['W', 'AH', 'N'], 'two': ['T', 'UW'], 'this': ['DH', 'IH', 'S'], 'is': ['IH', 'S'],
    'text': ['T', 'AE', 'K', 'S', 'T'], 'a': ['AH'], 'three': ['TH', 'R', 'IY'], 'four': ['F', 'AO', 'R'],
    'five': ['F', 'AA', 'EY', 'V'], 'six': ['S', 'IH', 'K', 'S'], 'seven': ['S', 'EH', 'V', 'EH', 'N'],
    'eight': ['EY', 'T'], 'nine': ['N', 'AA', 'EY', 'N'], 'ten': ['T', 'EH', 'N'],
    'robot': ['R', 'OW', 'B', 'AH', 'T'], 'voice': ['V', 'AO', 'Y', 'S'], 'i': ['AA', 'EY'],
    'am': ['AH', 'M'], 'and': ['AH', 'N', 'D'], 'single': ['S', 'IH', 'NG', 'G', 'AH', 'L'],
    'yes': ['Y', 'EH', 'S'], 'no': ['N', 'OW'], 'hi': ['HH', 'AA', 'EY'],
    'formant': ['F', 'AO', 'R', 'M', 'AH', 'N', 'T'], 'speech': ['S', 'P', 'IY', 'CH'],
    'synthesis': ['S', 'IH', 'N', 'TH', 'AH', 'S', 'IH', 'S'], 'tts': ['T', 'IY', 'T', 'IY', 'EH', 'S'],
    'computer': ['K', 'AH', 'M', 'P', 'Y', 'UW', 'T', 'ER'], 'please': ['P', 'L', 'IY', 'Z'],
    'thank': ['TH', 'AE', 'NG', 'K'], 'you': ['Y', 'UW'], 'good': ['G', 'UH', 'D'], 'morning': ['M', 'AO', 'R', 'N', 'IH', 'NG'],
    'afternoon': ['AE', 'F', 'T', 'ER', 'N', 'UW', 'N'], 'evening': ['IY', 'V', 'N', 'IH', 'NG'],
    'ship': ['SH', 'IH', 'P'], 'fish': ['F', 'IH', 'SH'], 'think': ['TH', 'IH', 'NG', 'K'],
    'sushi': ['S', 'UW', 'SH', 'IY'], 'zip': ['Z', 'IH', 'P'], 'measure': ['M', 'EH', 'ZH', 'ER'],
    'thin': ['TH', 'IH', 'N'], 'thick': ['TH', 'IH', 'K'], 'thistle': ['TH', 'IH', 'S', 'AH', 'L'],
}

def text_to_phonemes(text: str) -> List[str]:
    text = text.lower().strip()
    text = re.sub(r'[^a-z\s]', '', text)
    words = text.split()
    phoneme_sequence = ['SIL']
    for i, word in enumerate(words):
        phons = WORD_MAP.get(word, ['SIL'])
        phoneme_sequence.extend(phons)
        if i < len(words) - 1:
            phoneme_sequence.append('SIL')
    phoneme_sequence.append('SIL')
    return phoneme_sequence

def parse_phoneme_input(text: str) -> List[str]:
    """Parse raw phoneme input (space-separated ARPABET phonemes)"""
    tokens = text.strip().upper().split()
    phonemes = [tok if tok in VALID_PHONEMES else 'SIL' for tok in tokens]
    if not phonemes:
        return ['SIL', 'SIL']
    if phonemes[0] != 'SIL':
        phonemes.insert(0, 'SIL')
    if phonemes[-1] != 'SIL':
        phonemes.append('SIL')
    return phonemes

def parse_phoneme_spec(text: str, voice: Voice) -> List[Dict]:
    """Parse parameterized phoneme specification format"""
    specs = []
    for line_num, line in enumerate(text.splitlines(), 1):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split()
        if len(parts) < 4:
            continue
        ph_name = parts[0].upper()
        if ph_name not in PHONEME_TO_BYTE:
            continue
        try:
            duration = max(0.01, min(2.0, float(parts[1])))
            overlap = max(0.0, min(0.5, float(parts[2])))
            pitch_points = [float(p) for p in parts[3:]]
            if len(pitch_points) > 8:
                pitch_points = pitch_points[:8]
            ph_data = voice.get_phoneme_data(ph_name)
            f1 = ph_data.get('f1', 0.0) or 0.0
            f2 = ph_data.get('f2', 0.0) or 0.0
            f3 = ph_data.get('f3', 0.0) or 0.0
            specs.append({
                'phoneme': ph_name,
                'duration': duration,
                'overlap': overlap,
                'pitch_contour': pitch_points,
                'num_pitch_points': len(pitch_points),
                'f1': f1,
                'f2': f2,
                'f3': f3,
                'voiced': ph_name not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
            })
        except ValueError:
            continue
    return specs

def phonemes_to_spec(phonemes: List[str], voice: Voice, pitch_base: float = 115.0) -> List[Dict]:
    specs = []
    for i, ph in enumerate(phonemes):
        ph_data = voice.get_phoneme_data(ph)
        duration = ph_data.get('length', 0.14)
        overlap = 0.018 if ph in VOWELS and i < len(phonemes) - 1 else 0.008
        if ph == 'SIL':
            pitch = [0.0]
        elif ph in VOWELS:
            if i == len(phonemes) - 2:
                pitch = [pitch_base * 0.95, pitch_base * 0.90]
            elif i == 1:
                pitch = [pitch_base * 1.05, pitch_base * 1.10]
            else:
                pitch = [pitch_base]
        else:
            pitch = [pitch_base if ph_data.get('voiced', False) else 0.0]
        f1 = ph_data.get('f1', 0.0) or 0.0
        f2 = ph_data.get('f2', 0.0) or 0.0
        f3 = ph_data.get('f3', 0.0) or 0.0
        specs.append({
            'phoneme': ph,
            'duration': duration,
            'overlap': overlap,
            'pitch_contour': pitch,
            'num_pitch_points': len(pitch),
            'f1': f1,
            'f2': f2,
            'f3': f3,
            'voiced': ph not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
        })
    return specs

class FormantSynthesizer:
    def __init__(self, voice: Voice, sample_rate: int = smp):
        self.fs = sample_rate
        self.voice = voice
    
    def generate_glottal_pulse_train_contour(self, duration: float, pitch_contour: List[float]):
        n_samples = int(duration * self.fs)
        signal = np.zeros(n_samples)
        t = 0.0
        if not pitch_contour or all(p == 0 for p in pitch_contour):
            pitch_contour = [115.0]
        num_points = len(pitch_contour)
        while t < duration:
            t_norm = min(1.0, t / duration)
            if num_points == 1:
                f0 = pitch_contour[0]
            else:
                contour_pos = t_norm * (num_points - 1)
                idx_floor = int(contour_pos)
                frac = contour_pos - idx_floor
                if idx_floor >= num_points - 1:
                    f0 = pitch_contour[-1]
                else:
                    f0 = pitch_contour[idx_floor] * (1 - frac) + pitch_contour[idx_floor + 1] * frac
            f0 = max(50.0, min(400.0, f0))
            period_samples = self.fs / f0
            pulse_len = int(period_samples * 0.6)
            if pulse_len < 8:
                pulse_len = 8
            pulse = np.zeros(pulse_len)
            open_len = max(4, int(pulse_len * 0.4))
            pulse[:open_len] = -0.5 * (1 - np.cos(np.linspace(0, np.pi, open_len)))
            if pulse_len > open_len:
                close_len = pulse_len - open_len
                pulse[open_len:] = -0.1 * np.exp(-np.linspace(0, 5, close_len))
            start = int(t * self.fs)
            end = min(start + pulse_len, n_samples)
            if end > start:
                signal[start:end] += pulse[:end - start] * 0.6
            t += period_samples / self.fs
        peak = np.max(np.abs(signal))
        if peak > 0.1:
            signal = signal * (0.6 / peak)
        return signal
    
    def generate_shaped_noise(self, duration: float, phoneme: str, intensity: float = 0.25):
        n_samples = int(duration * self.fs)
        noise = np.random.randn(n_samples)
        if phoneme in {'S'}:
            b, a = sig.butter(6, [4000/(self.fs/2), 8500/(self.fs/2)], btype='band')
            noise = sig.filtfilt(b, a, noise)
            b2, a2 = sig.butter(4, 6500/(self.fs/2), btype='high')
            noise = sig.filtfilt(b2, a2, noise) * 1.3
        elif phoneme in {'SH', 'ZH'}:
            b, a = sig.butter(5, [2500/(self.fs/2), 6000/(self.fs/2)], btype='band')
            noise = sig.filtfilt(b, a, noise)
        elif phoneme in {'F', 'TH'}:
            b, a = sig.butter(4, 3500/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
        elif phoneme == 'HH':
            b, a = sig.butter(3, 2800/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
            noise += np.random.randn(n_samples) * 0.15
        elif phoneme in {'V', 'DH', 'Z'}:
            b, a = sig.butter(4, 4500/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
            voicing = np.sin(2 * np.pi * 120 * np.arange(n_samples) / self.fs) * 0.15
            noise = noise * 0.85 + voicing * 0.15
        else:
            b, a = sig.butter(4, 7500/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
        peak = np.max(np.abs(noise))
        if peak < 1e-6:
            noise = np.random.randn(n_samples) * intensity * 0.7
            peak = 1.0
        noise = noise * (intensity / peak)
        return noise[:n_samples]
    
    def stable_resonator(self, freq: float, bw: float):
        if freq <= 0:
            return np.array([1.0]), np.array([1.0])
        w0 = 2 * np.pi * freq / self.fs
        bw_rad = max(2 * np.pi * bw / self.fs, 2 * np.pi * 80 / self.fs)
        a1 = -2 * np.exp(-bw_rad/2) * np.cos(w0)
        a2 = np.exp(-bw_rad)
        b0 = np.sqrt(1 - a2)
        return np.array([b0]), np.array([1.0, a1, a2])
    
    def apply_formants_safe(self, signal: np.ndarray, f1: float, f2: float, f3: float) -> np.ndarray:
        b1, b2, b3 = 60, 90, 150
        for freq, bw in [(f1, b1), (f2, b2), (f3, b3)]:
            if freq and freq > 50:
                b, a = self.stable_resonator(freq, bw)
                signal = sig.lfilter(b, a, signal)
        peak = np.max(np.abs(signal))
        if peak > 4.0:
            signal = signal * (3.0 / peak)
        b, a = sig.butter(1, 900/(self.fs/2), btype='high')
        return sig.lfilter(b, a, signal)
    
    def synthesize_phoneme_direct(self, spec: Dict) -> np.ndarray:
        ph = spec['phoneme']
        dur = spec['duration']
        f1, f2, f3 = spec['f1'], spec['f2'], spec['f3']
        pitch_contour = spec['pitch_contour']
        voiced = spec['voiced']
        
        if ph == 'SIL':
            return np.zeros(int(dur * self.fs))
        
        if ph in STOPS:
            n_samples = int(dur * self.fs)
            out = np.zeros(n_samples)
            closure_end = int(n_samples * 0.82)
            burst_start = closure_end
            burst_len = min(200, n_samples - burst_start)
            if burst_len > 30:
                burst_noise = np.random.randn(burst_len)
                if f1 > 50:
                    b1, a1 = self.stable_resonator(f1, 150)
                    b2, a2 = self.stable_resonator(f2, 200)
                    burst_noise = sig.lfilter(b1, a1, burst_noise)
                    burst_noise = sig.lfilter(b2, a2, burst_noise)
                burst_env = np.hanning(burst_len) * 0.6
                out[burst_start:burst_start+burst_len] = burst_noise * burst_env
            
            if ph in {'P', 'T', 'K', 'CH'} and closure_end + burst_len < n_samples:
                aspir_start = burst_start + burst_len
                aspir_len = n_samples - aspir_start
                if aspir_len > 50:
                    aspiration = self.generate_shaped_noise(aspir_len/self.fs, 'HH', intensity=0.18)
                    b, a = sig.butter(2, 800/(self.fs/2), btype='high')
                    aspiration = sig.filtfilt(b, a, aspiration)
                    out[aspir_start:] = aspiration[:aspir_len] * 0.4
            elif ph in {'B', 'D', 'G'} and closure_end + burst_len < n_samples:
                voicing_start = burst_start + int(burst_len * 1.3)
                voicing_len = n_samples - voicing_start
                if voicing_len > 100:
                    voicing = self.generate_glottal_pulse_train_contour(voicing_len/self.fs, [115.0])
                    out[voicing_start:] = voicing[:voicing_len] * 0.35
            return out * 0.85
        
        if not voiced:
            if ph == 'S':
                intensity = 1.28
            elif ph == 'SH':
                intensity = 0.64
            elif ph in {'F', 'TH'}:
                intensity = 0.32
            else:
                intensity = 0.32
            source = self.generate_shaped_noise(dur, ph, intensity=intensity)
            if f1 > 50:
                if ph in {'S', 'SH'}:
                    source = self.apply_formants_safe(source, f1*0.7, f2*0.7, f3*0.7)
                else:
                    source = self.apply_formants_safe(source, f1, f2, f3)
            else:
                source = source * 0.45
            output = source
        else:
            source = self.generate_glottal_pulse_train_contour(dur, pitch_contour)
            if f1 > 50:
                output = self.apply_formants_safe(source, f1, f2, f3)
            else:
                output = source * 0.45
        
        n = len(output)
        env = np.ones(n)
        att = min(0.007, dur * 0.12)
        rel = min(0.018, dur * 0.28)
        att_s = int(att * self.fs)
        rel_s = int(rel * self.fs)
        if att_s > 0:
            env[:att_s] = np.linspace(0, 1, att_s)
        if rel_s > 0:
            env[-rel_s:] = np.linspace(1, 0.05, rel_s)
        output = output * env
        output = np.tanh(output * 1.15) * 0.93
        return output * 0.82
    
    def synthesize_from_specs(self, specs: List[Dict]) -> np.ndarray:
        if not specs:
            return np.zeros(0)
        
        total_duration = 0.0
        for spec in specs:
            total_duration += spec['duration']
        for i in range(len(specs) - 1):
            total_duration -= min(specs[i].get('overlap', 0.0), specs[i]['duration'])
        
        total_samples = int(total_duration * self.fs) + 10
        output = np.zeros(total_samples)
        current_pos = 0
        
        for i, spec in enumerate(specs):
            phoneme_audio = self.synthesize_phoneme_direct(spec)
            phoneme_samples = len(phoneme_audio)
            overlap_dur = spec.get('overlap', 0.0)
            if i == len(specs) - 1:
                overlap_dur = 0.0
            overlap_samples = min(
                int(overlap_dur * self.fs),
                phoneme_samples - 1,
                int(spec['duration'] * self.fs * 0.5)
            )
            end_pos = current_pos + phoneme_samples
            if end_pos > len(output):
                output = np.resize(output, end_pos + 1000)
            output[current_pos:end_pos] += phoneme_audio
            current_pos += (phoneme_samples - overlap_samples)
        
        actual_length = min(current_pos, len(output))
        audio = output[:actual_length]
        audio = np.tanh(audio * 1.25) * 0.94
        b, a = sig.butter(5, 5000/(self.fs/2), btype='low')
        audio = sig.filtfilt(b, a, audio)
        return audio

# Browser-compatible WAV generator (returns bytes)
def generate_wav_bytes(audio: np.ndarray, sr: int = smp) -> bytes:
    audio_int16 = np.clip(audio * 32767, -32768, 32767).astype(np.int16)
    buffer = BytesIO()
    with wave.open(buffer, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sr)
        wf.writeframes(audio_int16.tobytes())
    return buffer.getvalue()

# STRICT MODE HANDLING - NO AUTO-DETECTION
def synthesize_word_mode(text: str):
    phonemes = text_to_phonemes(text)
    specs = phonemes_to_spec(phonemes, VOICE_REGISTRY.current_voice)
    synth = FormantSynthesizer(VOICE_REGISTRY.current_voice)
    audio = synth.synthesize_from_specs(specs)
    return generate_wav_bytes(audio), "word"

def synthesize_phoneme_mode(text: str):
    phonemes = parse_phoneme_input(text)
    specs = phonemes_to_spec(phonemes, VOICE_REGISTRY.current_voice)
    synth = FormantSynthesizer(VOICE_REGISTRY.current_voice)
    audio = synth.synthesize_from_specs(specs)
    return generate_wav_bytes(audio), "phoneme"

def synthesize_spec_mode(text: str):
    specs = parse_phoneme_spec(text, VOICE_REGISTRY.current_voice)
    synth = FormantSynthesizer(VOICE_REGISTRY.current_voice)
    audio = synth.synthesize_from_specs(specs)
    return generate_wav_bytes(audio), "spec"
  </script>
  <script>
    // ======================
    // JAVASCRIPT FRONTEND
    // ======================
    let pyodide;
    let synthesizeWordFunc;
    let synthesizePhonemeFunc;
    let synthesizeSpecFunc;
    let currentAudioUrl = null;
    let currentWavBlob = null;
    const statusEl = document.getElementById('status');
    const synthesizeBtn = document.getElementById('synthesizeBtn');
    const playExampleBtn = document.getElementById('playExampleBtn');
    const downloadBtn = document.getElementById('downloadBtn');
    const playBtn = document.getElementById('playBtn');
    const pauseBtn = document.getElementById('pauseBtn');
    const audioPlayer = document.getElementById('audioPlayer');
    const textInput = document.getElementById('textInput');
    const phonemeInput = document.getElementById('phonemeInput');
    const specInput = document.getElementById('specInput');

    // Tab switching
    document.querySelectorAll('.tab').forEach(tab => {
      tab.addEventListener('click', () => {
        document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
        tab.classList.add('active');
        document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
        const tabName = tab.dataset.tab;
        document.getElementById(`${tabName}-tab`).classList.add('active');
      });
    });

    async function setupPyodide() {
      updateStatus("Loading Pyodide...", "status-loading");
      try {
        pyodide = await loadPyodide({
          indexURL: "https://cdn.jsdelivr.net/pyodide/v0.24.1/full/"
        });
        updateStatus("Installing packages...", "status-loading");
        await pyodide.loadPackage(["numpy", "scipy"]);
        const jimSource = document.getElementById('jim-source').textContent;
        updateStatus("Initializing speech engine...", "status-loading");
        pyodide.runPython(jimSource);
        pyodide.runPython(`
def js_synthesize_word(text):
    try:
        wav_bytes, mode = synthesize_word_mode(text)
        return wav_bytes, mode
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, "error"

def js_synthesize_phoneme(text):
    try:
        wav_bytes, mode = synthesize_phoneme_mode(text)
        return wav_bytes, mode
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, "error"

def js_synthesize_spec(text):
    try:
        wav_bytes, mode = synthesize_spec_mode(text)
        return wav_bytes, mode
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, "error"
        `);
        synthesizeWordFunc = pyodide.globals.get("js_synthesize_word");
        synthesizePhonemeFunc = pyodide.globals.get("js_synthesize_phoneme");
        synthesizeSpecFunc = pyodide.globals.get("js_synthesize_spec");
        updateStatus("‚úÖ Ready! Enter text and click Synthesize", "status-ready");
        synthesizeBtn.disabled = false;
        playExampleBtn.disabled = false;
      } catch (error) {
        console.error("Pyodide setup failed:", error);
        updateStatus(`‚ùå Initialization failed: ${error.message}`, "status-error");
      }
    }

    function updateStatus(message, className) {
      statusEl.textContent = message;
      statusEl.className = className;
    }

    async function synthesize(text, mode) {
      if (!synthesizeWordFunc || !synthesizePhonemeFunc || !synthesizeSpecFunc) {
        alert("Engine not ready yet. Please wait.");
        return null;
      }
      
      let func;
      switch(mode) {
        case 'word': func = synthesizeWordFunc; break;
        case 'phoneme': func = synthesizePhonemeFunc; break;
        case 'spec': func = synthesizeSpecFunc; break;
        default: return null;
      }
      
      try {
        updateStatus("üîä Synthesizing...", "status-processing");
        synthesizeBtn.disabled = true;
        playExampleBtn.disabled = true;
        downloadBtn.disabled = true;
        playBtn.disabled = true;
        pauseBtn.disabled = true;
        
        const result = func(text);
        if (!result || result[0] === null) {
          throw new Error("Synthesis returned null");
        }
        const [wavBytes, detectedMode] = result;
        const wavArray = wavBytes.toJs();
        wavBytes.destroy();
        
        if (!wavArray || wavArray.length < 44) {
          throw new Error(`Invalid WAV data: ${wavArray ? wavArray.length : 0} bytes`);
        }
        
        const wavBuffer = wavArray.buffer.slice(wavArray.byteOffset, wavArray.byteOffset + wavArray.byteLength);
        currentWavBlob = new Blob([wavBuffer], { type: 'audio/wav' });
        
        if (currentAudioUrl) {
          URL.revokeObjectURL(currentAudioUrl);
        }
        currentAudioUrl = URL.createObjectURL(currentWavBlob);
        audioPlayer.src = currentAudioUrl;
        audioPlayer.load();
        
        audioPlayer.onloadeddata = () => {
          downloadBtn.disabled = false;
          playBtn.disabled = false;
          pauseBtn.disabled = false;
          const modeNames = {
            'word': 'Word',
            'phoneme': 'Phoneme',
            'spec': 'Spec',
            'empty': 'Empty',
            'error': 'Error'
          };
          const sizeKB = (wavArray.length / 1024).toFixed(1);
          updateStatus(`‚úÖ ${modeNames[detectedMode] || detectedMode} synthesized! (${sizeKB} KB)`, "status-ready");
        };
        
        audioPlayer.onerror = (e) => {
          console.error("Audio load error:", e);
          updateStatus(`‚ùå Audio playback failed. Try downloading instead.`, "status-error");
          downloadBtn.disabled = false;
        };
        
        return currentAudioUrl;
      } catch (error) {
        console.error("Synthesis error:", error);
        updateStatus(`‚ùå Synthesis failed: ${error.message}`, "status-error");
        alert(`Synthesis failed: ${error.message}\nCheck console for details.`);
        return null;
      } finally {
        synthesizeBtn.disabled = false;
        playExampleBtn.disabled = false;
      }
    }

    function downloadWav() {
      if (!currentWavBlob) {
        alert("No audio to download. Please synthesize first.");
        return;
      }
      try {
        const activeTab = document.querySelector('.tab.active').dataset.tab;
        let filename = 'jim_output';
        if (activeTab === 'word') {
          const text = textInput.value.trim().substring(0, 30).replace(/[^\w\s]/g, '');
          filename = text ? `jim_${text.replace(/\s+/g, '_')}` : 'jim_output';
        } else if (activeTab === 'phoneme') {
          filename = 'jim_phonemes';
        } else {
          filename = 'jim_spec';
        }
        
        const downloadUrl = URL.createObjectURL(currentWavBlob);
        const link = document.createElement('a');
        link.href = downloadUrl;
        link.download = `${filename}.wav`;
        document.body.appendChild(link);
        link.click();
        document.body.removeChild(link);
        setTimeout(() => URL.revokeObjectURL(downloadUrl), 1000);
        updateStatus("üíæ Download started!", "status-ready");
      } catch (error) {
        console.error("Download error:", error);
        alert(`Download failed: ${error.message}`);
      }
    }

    // Playback controls
    playBtn.addEventListener('click', () => {
      if (audioPlayer.src) {
        audioPlayer.play().catch(e => {
          console.log("Autoplay prevented:", e);
          alert("Browser blocked autoplay. Click directly on the audio player controls to play.");
        });
      }
    });
    pauseBtn.addEventListener('click', () => {
      audioPlayer.pause();
    });

    // Synthesize button - STRICT MODE SELECTION
    synthesizeBtn.addEventListener('click', async () => {
      const activeTab = document.querySelector('.tab.active').dataset.tab;
      let text, mode;
      
      if (activeTab === 'word') {
        text = textInput.value.trim();
        mode = 'word';
      } else if (activeTab === 'phoneme') {
        text = phonemeInput.value.trim();
        mode = 'phoneme';
      } else {
        text = specInput.value.trim();
        mode = 'spec';
      }
      
      if (!text) {
        alert("Please enter some input first!");
        return;
      }
      
      await synthesize(text, mode);
    });

    // Example button
    playExampleBtn.addEventListener('click', async () => {
      const activeTab = document.querySelector('.tab.active').dataset.tab;
      if (activeTab === 'word') {
        textInput.value = "hello world this is a robot voice";
        await synthesize(textInput.value, 'word');
      } else if (activeTab === 'phoneme') {
        phonemeInput.value = "HH EH L OW SIL W ER L D SIL DH IH S SIL IH Z SIL AH SIL R OW B AA T SIL V AO Y S";
        await synthesize(phonemeInput.value, 'phoneme');
      } else {
        specInput.value = `# Hello World with pitch variation
HH       0.150  0.010  120.0
EH       0.140  0.018  115.0
L        0.120  0.008  115.0
OW       0.180  0.018  115.0 105.0
SIL      0.120  0.008  0.0
W        0.120  0.008  110.0
ER       0.140  0.018  115.0
L        0.120  0.008  115.0
D        0.068  0.008  0.0`;
        await synthesize(specInput.value, 'spec');
      }
    });

    // Download button
    downloadBtn.addEventListener('click', downloadWav);

    // Cleanup on page unload
    window.addEventListener('beforeunload', () => {
      if (currentAudioUrl) {
        URL.revokeObjectURL(currentAudioUrl);
      }
    });

    // Initialize
    window.addEventListener('load', setupPyodide);
  </script>
</body>
</html>
