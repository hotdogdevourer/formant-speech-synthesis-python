<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SSNJ Formant Speech Synthesizer</title>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 2rem auto;
            padding: 1.5rem;
            background: linear-gradient(135deg, #f5f7fa 0%, #e4edf5 100%);
        }
        h1 {
            text-align: center;
            margin-bottom: 1.5rem;
            color: #2c3e50;
            font-size: 2.2rem;
        }
        .container {
            background: white;
            border-radius: 16px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.08);
            padding: 2rem;
            margin-top: 1rem;
        }
        textarea {
            width: 100%;
            height: 160px;
            padding: 1rem;
            border: 2px solid #ddd;
            border-radius: 12px;
            font-size: 1rem;
            margin-bottom: 1.2rem;
            resize: vertical;
            transition: border-color 0.3s;
            font-family: monospace;
        }
        textarea:focus {
            outline: none;
            border-color: #4a6fa5;
            box-shadow: 0 0 0 3px rgba(74, 111, 165, 0.15);
        }
        .mode-indicator {
            text-align: center;
            margin-bottom: 1rem;
            padding: 0.6rem;
            border-radius: 10px;
            font-weight: 600;
            transition: all 0.3s ease;
        }
        .mode-word { background: #e8f5e9; color: #2e7d32; }
        .mode-phoneme { background: #e3f2fd; color: #1565c0; }
        .mode-spec { background: #f3e5f5; color: #4a148c; }
        .controls {
            display: flex;
            gap: 1rem;
            margin-bottom: 1.5rem;
            flex-wrap: wrap;
            align-items: stretch;
        }
        button {
            flex: 1;
            min-width: 120px;
            padding: 0.9rem 1.5rem;
            background: linear-gradient(to bottom, #4a6fa5, #2c3e50);
            color: white;
            border: none;
            border-radius: 12px;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(44, 62, 80, 0.3);
        }
        button:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(44, 62, 80, 0.4);
        }
        button:active:not(:disabled) {
            transform: translateY(0);
        }
        button:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        .download-btn {
            background: linear-gradient(to bottom, #27ae60, #219653);
            min-width: 140px;
        }
        .download-btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        .audio-container {
            margin-top: 1.5rem;
            text-align: center;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 1rem;
        }
        audio {
            width: 100%;
            max-width: 600px;
            margin-top: 0.5rem;
        }
        .audio-actions {
            display: flex;
            gap: 1rem;
            margin-top: 0.5rem;
            flex-wrap: wrap;
            justify-content: center;
        }
        .audio-actions button {
            padding: 0.7rem 1.2rem;
            font-size: 0.95rem;
            min-width: 100px;
        }
        #status {
            text-align: center;
            margin: 1.2rem 0;
            min-height: 1.8rem;
            font-weight: 500;
            padding: 0.5rem;
            border-radius: 8px;
        }
        .status-loading { color: #2980b9; background: #e3f2fd; }
        .status-ready { color: #27ae60; background: #e8f5e9; }
        .status-error { color: #e74c3c; background: #fadbd8; }
        .status-processing { color: #8e44ad; background: #f5f9fc; }
        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2980b9;
            padding: 1rem;
            border-radius: 0 8px 8px 0;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }
        .phoneme-guide, .spec-guide {
            padding: 1rem;
            border-radius: 0 8px 8px 0;
            margin: 1rem 0;
            font-family: monospace;
            font-size: 0.9rem;
        }
        .phoneme-guide {
            background: #e8f5e9;
            border-left: 4px solid #2e7d32;
        }
        .spec-guide {
            background: #f3e5f5;
            border-left: 4px solid #4a148c;
        }
        .limitations {
            font-size: 0.9rem;
            color: #7f8c8d;
            margin-top: 0.5rem;
            padding-top: 0.5rem;
            border-top: 1px dashed #bdc3c7;
        }
        .tabs {
            display: flex;
            margin-bottom: 1rem;
            gap: 0.5rem;
            flex-wrap: wrap;
        }
        .tab {
            padding: 0.6rem 1rem;
            background: #e0e7ff;
            border: none;
            border-radius: 8px 8px 0 0;
            cursor: pointer;
            font-weight: 600;
            flex: 1;
            min-width: 120px;
            text-align: center;
        }
        .tab.active {
            background: #4a6fa5;
            color: white;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
        .example-btn {
            background: linear-gradient(to bottom, #9c27b0, #6a1b9a);
        }
        .sample-rate-control {
            background: #f8f9fa;
            padding: 1.25rem;
            border-radius: 8px;
            margin-bottom: 1.25rem;
            border: 1px solid #ddd;
        }
        .sample-rate-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            font-weight: 600;
        }
        .sample-rate-slider {
            width: 100%;
            margin: 0.5rem 0;
        }
        .sample-rate-info {
            display: flex;
            justify-content: space-between;
            font-size: 0.9rem;
            color: #555;
            margin-top: 0.5rem;
        }
        .clamp-warning {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: #856404;
            padding: 0.5rem;
            background: #fff3cd;
            border-radius: 4px;
            border-left: 3px solid #ffc107;
        }
        @media (max-width: 600px) {
            .controls { flex-direction: column; }
            button { width: 100%; min-width: auto; }
            body { padding: 1rem; margin: 1rem; }
            .tabs { flex-direction: column; }
            .tab { border-radius: 8px; width: 100%; }
            .audio-actions { flex-direction: column; width: 100%; }
        }
    </style>
</head>
<body>
    <h1>ü§ñ Jim Formant Speech Synthesizer</h1>
    <div class="container">
        <div class="tabs">
            <button class="tab active" data-tab="word">üî§ Word Mode</button>
            <button class="tab" data-tab="phoneme">‚å®Ô∏è Phoneme Mode</button>
            <button class="tab" data-tab="spec">üéõÔ∏è Spec Mode</button>
        </div>
        
        <div id="word-tab" class="tab-content active">
            <div class="sample-rate-control">
                <div class="sample-rate-header">
                    <span>üéõÔ∏è Sample Rate (SMP): <span id="sampleRateValue">97000</span> Hz</span>
                    <span>Nyquist Limit: <strong id="nyquistValue">48500</strong> Hz</span>
                </div>
                <input type="range" id="sampleRateSlider" class="sample-rate-slider" min="8000" max="192000" step="100" value="97000">
                <div class="sample-rate-info">
                    <span>8 kHz</span>
                    <span>Clamp Limit: <strong id="clampValue">48000</strong> Hz (Nyquist - 500)</span>
                    <span>192 kHz</span>
                </div>
                <div class="clamp-warning">
                    ‚ö†Ô∏è All pitch values automatically clamped to <strong id="clampDisplay">48000</strong> Hz to prevent aliasing
                </div>
            </div>
            <textarea id="textInput" placeholder="Enter words (e.g., 'hello world robot')">hello world this is a robot voice</textarea>
            <div class="mode-indicator mode-word">üî§ Word Mode</div>
        </div>
        
        <div id="phoneme-tab" class="tab-content">
            <div class="sample-rate-control">
                <div class="sample-rate-header">
                    <span>üéõÔ∏è Sample Rate (SMP): <span id="sampleRateValue2">97000</span> Hz</span>
                    <span>Nyquist Limit: <strong id="nyquistValue2">48500</strong> Hz</span>
                </div>
                <input type="range" id="sampleRateSlider2" class="sample-rate-slider" min="8000" max="192000" step="100" value="97000">
                <div class="sample-rate-info">
                    <span>8 kHz</span>
                    <span>Clamp Limit: <strong id="clampValue2">48000</strong> Hz (Nyquist - 500)</span>
                    <span>192 kHz</span>
                </div>
                <div class="clamp-warning">
                    ‚ö†Ô∏è All pitch values automatically clamped to <strong id="clampDisplay2">48000</strong> Hz to prevent aliasing
                </div>
            </div>
            <textarea id="phonemeInput" placeholder="Enter phonemes separated by spaces (e.g., HH EH L OW SIL W ER L D)">HH EH L OW SIL W ER L D</textarea>
            <div class="phoneme-guide">
                <strong>Valid Phonemes (ARPABET):</strong><br>
                SIL, AH, AE, AA, AO, EH, EY, IH, IY, OW, UH, UW, ER,<br>
                B, D, G, P, T, K, M, N, NG, L, R, F, S, SH, TH, DH, V, Z, ZH, W, Y, HH, CH, JH<br>
                <strong>Tip:</strong> Use SIL between words for natural pauses
            </div>
        </div>
        
        <div id="spec-tab" class="tab-content">
            <div class="sample-rate-control">
                <div class="sample-rate-header">
                    <span>üéõÔ∏è Sample Rate (SMP): <span id="sampleRateValue3">97000</span> Hz</span>
                    <span>Nyquist Limit: <strong id="nyquistValue3">48500</strong> Hz</span>
                </div>
                <input type="range" id="sampleRateSlider3" class="sample-rate-slider" min="8000" max="192000" step="100" value="97000">
                <div class="sample-rate-info">
                    <span>8 kHz</span>
                    <span>Clamp Limit: <strong id="clampValue3">48000</strong> Hz (Nyquist - 500)</span>
                    <span>192 kHz</span>
                </div>
                <div class="clamp-warning">
                    ‚ö†Ô∏è All pitch values automatically clamped to <strong id="clampDisplay3">48000</strong> Hz to prevent aliasing
                </div>
            </div>
            <textarea id="specInput" placeholder="Enter spec format: PHON DUR OVERLAP PITCH0 PITCH1..."># PHONEME  DUR    OVRLP  PITCH_CONTOUR
HH       0.150  0.010  115.0
EH       0.140  0.018  115.0
L        0.120  0.008  115.0
OW       0.160  0.018  115.0 110.0
SIL      0.190  0.008  0.0
W        0.120  0.008  115.0
ER       0.140  0.018  115.0
L        0.120  0.008  115.0
D        0.068  0.008  0.0
SIL      0.190  0.000  0.0</textarea>
            <div class="spec-guide">
                <strong>Spec Format:</strong> <code>PHONEME DURATION OVERLAP PITCH0 [PITCH1 PITCH2 ...]</code><br>
                <strong>Fields:</strong><br>
                - PHONEME: ARPABET symbol (e.g., HH, EH, SIL)<br>
                - DUR: Duration in seconds (0.01-2.0)<br>
                - OVRLP: Overlap with next phoneme (0.0-0.5)<br>
                - PITCH: One or more pitch values in Hz (50-400)<br>
                <strong>Lines starting with # are comments</strong>
                <strong>Valid phonemes:</strong> SIL AH AE AA AO EH EY IH IY OW UH UW ER B D G P T K M N NG L R F S SH TH DH V Z ZH W Y HH CH JH
            </div>
        </div>
        
        <div class="controls">
            <button id="synthesizeBtn" disabled>‚ñ∂Ô∏è Synthesize</button>
            <button id="playExampleBtn" disabled class="example-btn">üéµ Example song</button>
            <button id="downloadBtn" disabled class="download-btn">üíæ Download WAV</button>
        </div>
        
        <div id="status" class="status-loading">Initializing Pyodide environment...</div>
        
        <div class="info-box">
            <strong>‚ÑπÔ∏è About Jim:</strong> Formant-based speech synthesizer running entirely in-browser via Pyodide.<br>
            Convert text ‚Üí phonemes ‚Üí formant synthesis ‚Üí WAV audio. No server required!
        </div>
        
        <div class="audio-container">
            <audio id="audioPlayer" controls>Your browser does not support the audio element.</audio>
            <div class="audio-actions">
                <button id="playBtn" disabled>‚ñ∂Ô∏è Play</button>
                <button id="pauseBtn" disabled>‚è∏Ô∏è Pause</button>
            </div>
        </div>
        
        <div class="limitations">
            ‚ö†Ô∏è <strong>Limitations:</strong> GitHub Pages has no server-side processing. All synthesis happens in your browser via WebAssembly (Pyodide). First load may take 15-30 seconds to initialize Python environment.
        </div>
    </div>
    
    <script type="text/python" id="jim-source">
import numpy as np
import scipy.signal as sig
import wave
import re
import json
from typing import Dict, List
from io import BytesIO

# ======================
# JIM.PY CORE CODE (Modified for browser execution)
# ======================

BYTE_TO_PHONEME = {
    0x00: 'SIL', 0x01: 'AH', 0x02: 'AE', 0x03: 'AA', 0x04: 'AO', 0x05: 'EH', 0x06: 'EY',
    0x07: 'IH', 0x08: 'IY', 0x09: 'OW', 0x0A: 'UH', 0x0B: 'UW', 0x0C: 'ER', 0x0D: 'B',
    0x0E: 'D', 0x0F: 'G', 0x10: 'P', 0x11: 'T', 0x12: 'K', 0x13: 'M', 0x14: 'N', 0x15: 'NG',
    0x16: 'L', 0x17: 'R', 0x18: 'F', 0x19: 'S', 0x1A: 'SH', 0x1B: 'TH', 0x1C: 'DH', 0x1D: 'V',
    0x1E: 'Z', 0x1F: 'ZH', 0x20: 'W', 0x21: 'Y', 0x22: 'HH', 0x23: 'CH', 0x24: 'JH',
}

PHONEME_TO_BYTE = {v: k for k, v in BYTE_TO_PHONEME.items()}
VALID_PHONEMES = set(PHONEME_TO_BYTE.keys())
VOWELS = {'AH','AE','AA','AO','EH','EY','IH','IY','OW','UH','UW','ER'}
STOPS = {'P','T','K','B','D','G','CH'}
FRICATIVES_UNVOICED = {'F','S','SH','TH','HH'}
FRICATIVES_VOICED = {'V','Z','ZH','DH'}

class Voice:
    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self.phonemes: Dict[str, Dict] = {}
    
    def get_phoneme_data(self, phoneme: str) -> Dict:
        if phoneme.endswith('_FINAL'):
            base_ph = phoneme.replace('_FINAL', '')
            data = self.phonemes.get(base_ph, self.phonemes.get('SIL', {})).copy()
            if base_ph in VOWELS:
                data['length'] = min(data.get('length', 0.14) * 1.4, 0.35)
            return data
        return self.phonemes.get(phoneme, self.phonemes.get('SIL', {}))
    
    def save(self, filepath: str) -> None:
        pass
    
    @classmethod
    def load(cls, filepath: str) -> 'Voice':
        pass

class DefaultVoice(Voice):
    def __init__(self):
        super().__init__("Default", "Built-in robotic voice")
        self.phonemes = {
            'AH': {'f1': 700, 'f2': 1100, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AE': {'f1': 650, 'f2': 1250, 'f3': 2500, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AA': {'f1': 620, 'f2': 1180, 'f3': 2550, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AO': {'f1': 550, 'f2': 850, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'EH': {'f1': 530, 'f2': 1700, 'f3': 2450, 'f4': 115, 'length': 0.14, 'voiced': True},
            'EY': {'f1': 400, 'f2': 2100, 'f3': 2800, 'f4': 115, 'length': 0.14, 'voiced': True},
            'IH': {'f1': 420, 'f2': 1950, 'f3': 2500, 'f4': 115, 'length': 0.14, 'voiced': True},
            'IY': {'f1': 300, 'f2': 2250, 'f3': 3000, 'f4': 115, 'length': 0.14, 'voiced': True},
            'OW': {'f1': 450, 'f2': 900, 'f3': 2350, 'f4': 115, 'length': 0.14, 'voiced': True},
            'UH': {'f1': 400, 'f2': 650, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'UW': {'f1': 330, 'f2': 900, 'f3': 2200, 'f4': 115, 'length': 0.14, 'voiced': True},
            'ER': {'f1': 480, 'f2': 1180, 'f3': 1650, 'f4': 115, 'length': 0.14, 'voiced': True},
            'M':  {'f1': 350, 'f2': 1050, 'f3': 2250, 'f4': 115, 'length': 0.12, 'voiced': True},
            'N':  {'f1': 320, 'f2': 1150, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'NG': {'f1': 280, 'f2': 950, 'f3': 2350, 'f4': 115, 'length': 0.12, 'voiced': True},
            'L':  {'f1': 400, 'f2': 1150, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'R':  {'f1': 450, 'f2': 1250, 'f3': 1500, 'f4': 115, 'length': 0.12, 'voiced': True},
            'DH': {'f1': 380, 'f2': 1650, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'V':  {'f1': 380, 'f2': 1550, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'Z':  {'f1': 380, 'f2': 1750, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'ZH': {'f1': 380, 'f2': 1450, 'f3': 2250, 'f4': 115, 'length': 0.12, 'voiced': True},
            'W':  {'f1': 350, 'f2': 700, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'Y':  {'f1': 350, 'f2': 2050, 'f3': 2650, 'f4': 115, 'length': 0.12, 'voiced': True},
            'JH': {'f1': 400, 'f2': 1650, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'B':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'D':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'G':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'P':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'T':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'K':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'F':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'S':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'SH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'TH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'HH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'CH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'SIL': {'f1': 0, 'f2': 0, 'f3': 0, 'f4': 0, 'length': 0.19, 'voiced': 'silence'},
        }

class VoiceRegistry:
    def __init__(self):
        self.voices: Dict[str, Voice] = {'Default': DefaultVoice()}
        self.current_voice: Voice = self.voices['Default']
    
    def set_current_voice(self, name: str) -> bool:
        if name in self.voices:
            self.current_voice = self.voices[name]
            return True
        return False
    
    def list_voices(self) -> Dict[str, Voice]:
        return self.voices

VOICE_REGISTRY = VoiceRegistry()

WORD_MAP = {
    'hello': ['HH', 'EH', 'L', 'AO', 'OW'], 'world': ['W', 'ER', 'L', 'D'], 'test': ['T', 'EH', 'S', 'T'],
    'one': ['W', 'AH', 'N'], 'two': ['T', 'UW'], 'this': ['DH', 'IH', 'S'], 'is': ['IH', 'S'],
    'text': ['T', 'AE', 'K', 'S', 'T'], 'a': ['AH'], 'three': ['TH', 'R', 'IY'], 'four': ['F', 'AO', 'R'],
    'five': ['F', 'AA', 'EY', 'V'], 'six': ['S', 'IH', 'K', 'S'], 'seven': ['S', 'EH', 'V', 'EH', 'N'],
    'eight': ['EY', 'T'], 'nine': ['N', 'AA', 'EY', 'N'], 'ten': ['T', 'EH', 'N'],
    'robot': ['R', 'OW', 'B', 'AH', 'T'], 'voice': ['V', 'AO', 'Y', 'S'], 'i': ['AA', 'EY'],
    'am': ['AH', 'M'], 'and': ['AH', 'N', 'D'], 'single': ['S', 'IH', 'NG', 'G', 'AH', 'L'],
    'yes': ['Y', 'EH', 'S'], 'no': ['N', 'OW'], 'hi': ['HH', 'AA', 'EY'],
    'formant': ['F', 'AO', 'R', 'M', 'AH', 'N', 'T'], 'speech': ['S', 'P', 'IY', 'CH'],
    'synthesis': ['S', 'IH', 'N', 'TH', 'AH', 'S', 'IH', 'S'], 'tts': ['T', 'IY', 'T', 'IY', 'EH', 'S'],
    'computer': ['K', 'AH', 'M', 'P', 'Y', 'UW', 'T', 'ER'], 'please': ['P', 'L', 'IY', 'Z'],
    'thank': ['TH', 'AE', 'NG', 'K'], 'you': ['Y', 'UW'], 'good': ['G', 'UH', 'D'], 'morning': ['M', 'AO', 'R', 'N', 'IH', 'NG'],
    'afternoon': ['AE', 'F', 'T', 'ER', 'N', 'UW', 'N'], 'evening': ['IY', 'V', 'N', 'IH', 'NG'],
    'ship': ['SH', 'IH', 'P'], 'fish': ['F', 'IH', 'SH'], 'think': ['TH', 'IH', 'NG', 'K'],
    'sushi': ['S', 'UW', 'SH', 'IY'], 'zip': ['Z', 'IH', 'P'], 'measure': ['M', 'EH', 'ZH', 'ER'],
    'thin': ['TH', 'IH', 'N'], 'thick': ['TH', 'IH', 'K'], 'thistle': ['TH', 'IH', 'S', 'AH', 'L'],
}

def text_to_phonemes(text: str) -> List[str]:
    text = text.lower().strip()
    text = re.sub(r'[^a-z\s]', '', text)
    words = text.split()
    phoneme_sequence = ['SIL']
    for i, word in enumerate(words):
        phons = WORD_MAP.get(word, ['SIL'])
        phoneme_sequence.extend(phons)
        if i < len(words) - 1:
            phoneme_sequence.append('SIL')
    phoneme_sequence.append('SIL')
    return phoneme_sequence

def parse_phoneme_input(text: str) -> List[str]:
    tokens = text.strip().upper().split()
    phonemes = [tok if tok in VALID_PHONEMES else 'SIL' for tok in tokens]
    if not phonemes:
        return ['SIL', 'SIL']
    if phonemes[0] != 'SIL':
        phonemes.insert(0, 'SIL')
    if phonemes[-1] != 'SIL':
        phonemes.append('SIL')
    return phonemes

def parse_phoneme_spec(text: str, voice: Voice) -> List[Dict]:
    specs = []
    for line_num, line in enumerate(text.splitlines(), 1):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split()
        if len(parts) < 4:
            continue
        ph_name = parts[0].upper()
        if ph_name not in PHONEME_TO_BYTE:
            continue
        try:
            duration = max(0.01, min(2.0, float(parts[1])))
            overlap = max(0.0, min(0.5, float(parts[2])))
            pitch_points = [float(p) for p in parts[3:]]
            if len(pitch_points) > 8:
                pitch_points = pitch_points[:8]
            ph_data = voice.get_phoneme_data(ph_name)
            f1 = ph_data.get('f1', 0.0) or 0.0
            f2 = ph_data.get('f2', 0.0) or 0.0
            f3 = ph_data.get('f3', 0.0) or 0.0
            specs.append({
                'phoneme': ph_name,
                'duration': duration,
                'overlap': overlap,
                'pitch_contour': pitch_points,
                'num_pitch_points': len(pitch_points),
                'f1': f1,
                'f2': f2,
                'f3': f3,
                'voiced': ph_name not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
            })
        except ValueError:
            continue
    return specs

def phonemes_to_spec(phonemes: List[str], voice: Voice, pitch_base: float = 115.0) -> List[Dict]:
    specs = []
    for i, ph in enumerate(phonemes):
        ph_data = voice.get_phoneme_data(ph)
        duration = ph_data.get('length', 0.14)
        overlap = 0.018 if ph in VOWELS and i < len(phonemes) - 1 else 0.008
        if ph == 'SIL':
            pitch = [0.0]
        elif ph in VOWELS:
            if i == len(phonemes) - 2:
                pitch = [pitch_base * 0.95, pitch_base * 0.90]
            elif i == 1:
                pitch = [pitch_base * 1.05, pitch_base * 1.10]
            else:
                pitch = [pitch_base]
        else:
            pitch = [pitch_base if ph_data.get('voiced', False) else 0.0]
        f1 = ph_data.get('f1', 0.0) or 0.0
        f2 = ph_data.get('f2', 0.0) or 0.0
        f3 = ph_data.get('f3', 0.0) or 0.0
        specs.append({
            'phoneme': ph,
            'duration': duration,
            'overlap': overlap,
            'pitch_contour': pitch,
            'num_pitch_points': len(pitch),
            'f1': f1,
            'f2': f2,
            'f3': f3,
            'voiced': ph not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
        })
    return specs

class FormantSynthesizer:
    def __init__(self, voice: Voice, sample_rate: int = 97000):
        self.fs = sample_rate
        self.voice = voice
        self.nyquist = self.fs / 2
        self.ref_fs = 97000
    
    def scale_freq(self, freq: float) -> float:
        scaled = freq * (self.fs / self.ref_fs)
        return max(20.0, min(scaled, self.nyquist * 0.99))
    
    def generate_glottal_pulse_train_contour(self, duration: float, pitch_contour: List[float]):
        n_samples = int(duration * self.fs)
        signal = np.zeros(n_samples)
        t = 0.0
        if not pitch_contour or all(p == 0 for p in pitch_contour):
            pitch_contour = [115.0]
        num_points = len(pitch_contour)
        while t < duration:
            t_norm = min(1.0, t / duration)
            if num_points == 1:
                f0 = pitch_contour[0]
            else:
                contour_pos = t_norm * (num_points - 1)
                idx_floor = int(contour_pos)
                frac = contour_pos - idx_floor
                if idx_floor >= num_points - 1:
                    f0 = pitch_contour[-1]
                else:
                    f0 = pitch_contour[idx_floor] * (1 - frac) + pitch_contour[idx_floor + 1] * frac
            
            nyquist_limit = (self.fs / 2) - 500
            f0 = max(50.0, min(nyquist_limit, f0))
            
            period_samples = self.fs / f0
            pulse_len = int(period_samples * 0.6)
            if pulse_len < 8:
                pulse_len = 8
            pulse = np.zeros(pulse_len)
            open_len = max(4, int(pulse_len * 0.4))
            pulse[:open_len] = -0.5 * (1 - np.cos(np.linspace(0, np.pi, open_len)))
            if pulse_len > open_len:
                close_len = pulse_len - open_len
                pulse[open_len:] = -0.1 * np.exp(-np.linspace(0, 5, close_len))
            start = int(t * self.fs)
            end = min(start + pulse_len, n_samples)
            if end > start:
                signal[start:end] += pulse[:end - start] * 0.6
            t += period_samples / self.fs
        
        peak = np.max(np.abs(signal))
        if peak > 0.1:
            signal = signal * (0.6 / peak)
        return signal
    
    def generate_shaped_noise(self, duration: float, phoneme: str, intensity: float = 0.25):
        n_samples = int(duration * self.fs)
        noise = np.random.randn(n_samples)
        
        if phoneme in {'S'}:
            low_f = self.scale_freq(4000)
            high_f = self.scale_freq(8500)
            b, a = sig.butter(6, [low_f/self.nyquist, high_f/self.nyquist], btype='band')
            noise = sig.filtfilt(b, a, noise)
            cutoff_f = self.scale_freq(6500)
            b2, a2 = sig.butter(4, cutoff_f/self.nyquist, btype='high')
            noise = sig.filtfilt(b2, a2, noise) * 1.3
        elif phoneme in {'SH', 'ZH'}:
            low_f = self.scale_freq(2500)
            high_f = self.scale_freq(6000)
            b, a = sig.butter(5, [low_f/self.nyquist, high_f/self.nyquist], btype='band')
            noise = sig.filtfilt(b, a, noise)
        elif phoneme in {'F', 'TH'}:
            cutoff_f = self.scale_freq(3500)
            b, a = sig.butter(4, cutoff_f/self.nyquist, btype='low')
            noise = sig.filtfilt(b, a, noise)
        elif phoneme == 'HH':
            cutoff_f = self.scale_freq(2800)
            b, a = sig.butter(3, cutoff_f/self.nyquist, btype='low')
            noise = sig.filtfilt(b, a, noise)
            noise += np.random.randn(n_samples) * 0.15
        elif phoneme in {'V', 'DH', 'Z'}:
            cutoff_f = self.scale_freq(4500)
            b, a = sig.butter(4, cutoff_f/self.nyquist, btype='low')
            noise = sig.filtfilt(b, a, noise)
            voicing = np.sin(2 * np.pi * 120 * np.arange(n_samples) / self.fs) * 0.15
            noise = noise * 0.85 + voicing * 0.15
        else:
            cutoff_f = self.scale_freq(7500)
            b, a = sig.butter(4, cutoff_f/self.nyquist, btype='low')
            noise = sig.filtfilt(b, a, noise)
        
        peak = np.max(np.abs(noise))
        if peak < 1e-6:
            noise = np.random.randn(n_samples) * intensity * 0.7
            peak = 1.0
        noise = noise * (intensity / peak)
        return noise[:n_samples]
    
    def stable_resonator(self, freq: float, bw: float):
        if freq <= 0:
            return np.array([1.0]), np.array([1.0])
        w0 = 2 * np.pi * freq / self.fs
        bw_rad = max(2 * np.pi * bw / self.fs, 2 * np.pi * 80 / self.fs)
        a1 = -2 * np.exp(-bw_rad/2) * np.cos(w0)
        a2 = np.exp(-bw_rad)
        b0 = np.sqrt(1 - a2)
        return np.array([b0]), np.array([1.0, a1, a2])
    
    def apply_formants_safe(self, signal: np.ndarray, f1: float, f2: float, f3: float) -> np.ndarray:
        b1, b2, b3 = 60, 90, 150
        for freq, bw in [(f1, b1), (f2, b2), (f3, b3)]:
            if freq and freq > 50:
                b, a = self.stable_resonator(freq, bw)
                signal = sig.lfilter(b, a, signal)
        peak = np.max(np.abs(signal))
        if peak > 4.0:
            signal = signal * (3.0 / peak)
        cutoff_f = self.scale_freq(900)
        b, a = sig.butter(1, cutoff_f/self.nyquist, btype='high')
        return sig.lfilter(b, a, signal)
    
    def synthesize_phoneme_direct(self, spec: Dict) -> np.ndarray:
        ph = spec['phoneme']
        dur = spec['duration']
        f1, f2, f3 = spec['f1'], spec['f2'], spec['f3']
        pitch_contour = spec['pitch_contour']
        voiced = spec['voiced']
        
        if ph == 'SIL':
            return np.zeros(int(dur * self.fs))
        
        if ph in STOPS:
            n_samples = int(dur * self.fs)
            out = np.zeros(n_samples)
            closure_end = int(n_samples * 0.82)
            burst_start = closure_end
            burst_len = min(200, n_samples - burst_start)
            if burst_len > 30:
                burst_noise = np.random.randn(burst_len)
                if f1 > 50:
                    b1, a1 = self.stable_resonator(f1, 150)
                    b2, a2 = self.stable_resonator(f2, 200)
                    burst_noise = sig.lfilter(b1, a1, burst_noise)
                    burst_noise = sig.lfilter(b2, a2, burst_noise)
                burst_env = np.hanning(burst_len) * 0.6
                out[burst_start:burst_start+burst_len] = burst_noise * burst_env
            
            if ph in {'P', 'T', 'K', 'CH'} and closure_end + burst_len < n_samples:
                aspir_start = burst_start + burst_len
                aspir_len = n_samples - aspir_start
                if aspir_len > 50:
                    aspiration = self.generate_shaped_noise(aspir_len/self.fs, 'HH', intensity=0.18)
                    b, a = sig.butter(2, 800/(self.fs/2), btype='high')
                    aspiration = sig.filtfilt(b, a, aspiration)
                    out[aspir_start:] = aspiration[:aspir_len] * 0.4
            elif ph in {'B', 'D', 'G'} and closure_end + burst_len < n_samples:
                voicing_start = burst_start + int(burst_len * 1.3)
                voicing_len = n_samples - voicing_start
                if voicing_len > 100:
                    voicing = self.generate_glottal_pulse_train_contour(voicing_len/self.fs, [115.0])
                    out[voicing_start:] = voicing[:voicing_len] * 0.35
            return out * 0.85
        
        if not voiced:
            if ph == 'S':
                intensity = 1.28
            elif ph == 'SH':
                intensity = 0.64
            elif ph in {'F', 'TH'}:
                intensity = 0.32
            else:
                intensity = 0.32
            source = self.generate_shaped_noise(dur, ph, intensity=intensity)
            if f1 > 50:
                if ph in {'S', 'SH'}:
                    source = self.apply_formants_safe(source, f1*0.7, f2*0.7, f3*0.7)
                else:
                    source = self.apply_formants_safe(source, f1, f2, f3)
            else:
                source = source * 0.45
            output = source
        else:
            source = self.generate_glottal_pulse_train_contour(dur, pitch_contour)
            if f1 > 50:
                output = self.apply_formants_safe(source, f1, f2, f3)
            else:
                output = source * 0.45
        
        n = len(output)
        env = np.ones(n)
        att = min(0.007, dur * 0.12)
        rel = min(0.018, dur * 0.28)
        att_s = int(att * self.fs)
        rel_s = int(rel * self.fs)
        if att_s > 0:
            env[:att_s] = np.linspace(0, 1, att_s)
        if rel_s > 0:
            env[-rel_s:] = np.linspace(1, 0.05, rel_s)
        output = output * env
        output = np.tanh(output * 1.15) * 0.93
        return output * 0.82
    
    def synthesize_from_specs(self, specs: List[Dict]) -> np.ndarray:
        if not specs:
            return np.zeros(0)
        
        total_duration = 0.0
        for spec in specs:
            total_duration += spec['duration']
        for i in range(len(specs) - 1):
            total_duration -= min(specs[i].get('overlap', 0.0), specs[i]['duration'])
        
        total_samples = int(total_duration * self.fs) + 10
        output = np.zeros(total_samples)
        current_pos = 0
        
        for i, spec in enumerate(specs):
            phoneme_audio = self.synthesize_phoneme_direct(spec)
            phoneme_samples = len(phoneme_audio)
            overlap_dur = spec.get('overlap', 0.0)
            if i == len(specs) - 1:
                overlap_dur = 0.0
            overlap_samples = min(
                int(overlap_dur * self.fs),
                phoneme_samples - 1,
                int(spec['duration'] * self.fs * 0.5)
            )
            end_pos = current_pos + phoneme_samples
            if end_pos > len(output):
                output = np.resize(output, end_pos + 1000)
            output[current_pos:end_pos] += phoneme_audio
            current_pos += (phoneme_samples - overlap_samples)
        
        actual_length = min(current_pos, len(output))
        audio = output[:actual_length]
        audio = np.tanh(audio * 1.25) * 0.94
        cutoff_f = self.scale_freq(5000)
        b, a = sig.butter(5, cutoff_f/self.nyquist, btype='low')
        audio = sig.filtfilt(b, a, audio)
        return audio

def generate_wav_bytes(audio: np.ndarray, sr: int) -> bytes:
    audio_int16 = np.clip(audio * 32767, -32768, 32767).astype(np.int16)
    buffer = BytesIO()
    with wave.open(buffer, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sr)
        wf.writeframes(audio_int16.tobytes())
    return buffer.getvalue()

def synthesize_word_mode(text: str, sample_rate: int = 97000):
    phonemes = text_to_phonemes(text)
    specs = phonemes_to_spec(phonemes, VOICE_REGISTRY.current_voice)
    synth = FormantSynthesizer(VOICE_REGISTRY.current_voice, sample_rate=sample_rate)
    audio = synth.synthesize_from_specs(specs)
    return generate_wav_bytes(audio, sr=sample_rate), "word"

def synthesize_phoneme_mode(text: str, sample_rate: int = 97000):
    phonemes = parse_phoneme_input(text)
    specs = phonemes_to_spec(phonemes, VOICE_REGISTRY.current_voice)
    synth = FormantSynthesizer(VOICE_REGISTRY.current_voice, sample_rate=sample_rate)
    audio = synth.synthesize_from_specs(specs)
    return generate_wav_bytes(audio, sr=sample_rate), "phoneme"

def synthesize_spec_mode(text: str, sample_rate: int = 97000):
    specs = parse_phoneme_spec(text, VOICE_REGISTRY.current_voice)
    synth = FormantSynthesizer(VOICE_REGISTRY.current_voice, sample_rate=sample_rate)
    audio = synth.synthesize_from_specs(specs)
    return generate_wav_bytes(audio, sr=sample_rate), "spec"
    </script>
    
    <script>
        // ======================
        // JAVASCRIPT FRONTEND
        // ======================
        let pyodide;
        let synthesizeWordFunc;
        let synthesizePhonemeFunc;
        let synthesizeSpecFunc;
        let currentAudioUrl = null;
        let currentWavBlob = null;
        let currentSampleRate = 97000;
        
        const statusEl = document.getElementById('status');
        const synthesizeBtn = document.getElementById('synthesizeBtn');
        const playExampleBtn = document.getElementById('playExampleBtn');
        const downloadBtn = document.getElementById('downloadBtn');
        const playBtn = document.getElementById('playBtn');
        const pauseBtn = document.getElementById('pauseBtn');
        const audioPlayer = document.getElementById('audioPlayer');
        const textInput = document.getElementById('textInput');
        const phonemeInput = document.getElementById('phonemeInput');
        const specInput = document.getElementById('specInput');
        
        // Sample Rate Sliders for all three modes
        const sliders = {
            word: {
                slider: document.getElementById('sampleRateSlider'),
                rate: document.getElementById('sampleRateValue'),
                nyquist: document.getElementById('nyquistValue'),
                clamp: document.getElementById('clampValue'),
                display: document.getElementById('clampDisplay')
            },
            phoneme: {
                slider: document.getElementById('sampleRateSlider2'),
                rate: document.getElementById('sampleRateValue2'),
                nyquist: document.getElementById('nyquistValue2'),
                clamp: document.getElementById('clampValue2'),
                display: document.getElementById('clampDisplay2')
            },
            spec: {
                slider: document.getElementById('sampleRateSlider3'),
                rate: document.getElementById('sampleRateValue3'),
                nyquist: document.getElementById('nyquistValue3'),
                clamp: document.getElementById('clampValue3'),
                display: document.getElementById('clampDisplay3')
            }
        };
        
        // Update all sliders synchronously
        function updateSampleRateDisplays(value) {
            currentSampleRate = parseInt(value);
            for (const mode in sliders) {
                const s = sliders[mode];
                s.rate.textContent = currentSampleRate;
                const nyquist = currentSampleRate / 2;
                const clampLimit = nyquist - 500;
                s.nyquist.textContent = Math.round(nyquist);
                s.clamp.textContent = Math.round(clampLimit);
                s.display.textContent = Math.round(clampLimit);
            }
        }
        
        // Sync all sliders together
        for (const mode in sliders) {
            const slider = sliders[mode].slider;
            if (slider) {
                slider.addEventListener('input', (e) => {
                    // Update all sliders to match
                    for (const m in sliders) {
                        sliders[m].slider.value = e.target.value;
                    }
                    updateSampleRateDisplays(e.target.value);
                });
            }
        }
        
        // Initialize displays
        updateSampleRateDisplays(97000);
        
        // Tab switching
        document.querySelectorAll('.tab').forEach(tab => {
            tab.addEventListener('click', () => {
                document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
                tab.classList.add('active');
                document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
                const tabName = tab.dataset.tab;
                document.getElementById(`${tabName}-tab`).classList.add('active');
            });
        });
        
        async function setupPyodide() {
            updateStatus("Loading Pyodide...", "status-loading");
            try {
                pyodide = await loadPyodide({
                    indexURL: "https://cdn.jsdelivr.net/pyodide/v0.24.1/full/"
                });
                updateStatus("Installing packages...", "status-loading");
                await pyodide.loadPackage(["numpy", "scipy"]);
                const jimSource = document.getElementById('jim-source').textContent;
                updateStatus("Initializing speech engine...", "status-loading");
                pyodide.runPython(jimSource);
                pyodide.runPython(`
def js_synthesize_word(text, sample_rate=97000):
    try:
        wav_bytes, mode = synthesize_word_mode(text, sample_rate)
        return wav_bytes, mode
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, "error"

def js_synthesize_phoneme(text, sample_rate=97000):
    try:
        wav_bytes, mode = synthesize_phoneme_mode(text, sample_rate)
        return wav_bytes, mode
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, "error"

def js_synthesize_spec(text, sample_rate=97000):
    try:
        wav_bytes, mode = synthesize_spec_mode(text, sample_rate)
        return wav_bytes, mode
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, "error"
`);
                synthesizeWordFunc = pyodide.globals.get("js_synthesize_word");
                synthesizePhonemeFunc = pyodide.globals.get("js_synthesize_phoneme");
                synthesizeSpecFunc = pyodide.globals.get("js_synthesize_spec");
                updateStatus("‚úÖ Ready! Enter text and click Synthesize", "status-ready");
                synthesizeBtn.disabled = false;
                playExampleBtn.disabled = false;
            } catch (error) {
                console.error("Pyodide setup failed:", error);
                updateStatus(`‚ùå Initialization failed: ${error.message}`, "status-error");
            }
        }
        
        function updateStatus(message, className) {
            statusEl.textContent = message;
            statusEl.className = className;
        }
        
        async function synthesize(text, mode) {
            if (!synthesizeWordFunc || !synthesizePhonemeFunc || !synthesizeSpecFunc) {
                alert("Engine not ready yet. Please wait.");
                return null;
            }
            
            let func;
            let sampleRate = currentSampleRate;
            
            switch(mode) {
                case 'word': 
                    func = synthesizeWordFunc; 
                    break;
                case 'phoneme': 
                    func = synthesizePhonemeFunc; 
                    break;
                case 'spec': 
                    func = synthesizeSpecFunc;
                    break;
                default: 
                    return null;
            }
            
            try {
                updateStatus("üîä Synthesizing...", "status-processing");
                synthesizeBtn.disabled = true;
                playExampleBtn.disabled = true;
                downloadBtn.disabled = true;
                playBtn.disabled = true;
                pauseBtn.disabled = true;
                
                const result = func(text, sampleRate);
                
                if (!result || result[0] === null) {
                    throw new Error("Synthesis returned null");
                }
                
                const [wavBytes, detectedMode] = result;
                const wavArray = wavBytes.toJs();
                wavBytes.destroy();
                
                if (!wavArray || wavArray.length < 44) {
                    throw new Error(`Invalid WAV data: ${wavArray ? wavArray.length : 0} bytes`);
                }
                
                const wavBuffer = wavArray.buffer.slice(wavArray.byteOffset, wavArray.byteOffset + wavArray.byteLength);
                currentWavBlob = new Blob([wavBuffer], { type: 'audio/wav' });
                
                if (currentAudioUrl) {
                    URL.revokeObjectURL(currentAudioUrl);
                }
                
                currentAudioUrl = URL.createObjectURL(currentWavBlob);
                audioPlayer.src = currentAudioUrl;
                audioPlayer.load();
                
                audioPlayer.onloadeddata = () => {
                    downloadBtn.disabled = false;
                    playBtn.disabled = false;
                    pauseBtn.disabled = false;
                    const modeNames = {
                        'word': 'Word',
                        'phoneme': 'Phoneme',
                        'spec': 'Spec',
                        'empty': 'Empty',
                        'error': 'Error'
                    };
                    const sizeKB = (wavArray.length / 1024).toFixed(1);
                    updateStatus(`‚úÖ ${modeNames[detectedMode] || detectedMode} synthesized! (${sizeKB} KB) @ ${sampleRate} Hz`, "status-ready");
                };
                
                audioPlayer.onerror = (e) => {
                    console.error("Audio load error:", e);
                    updateStatus(`‚ùå Audio playback failed. Try downloading instead.`, "status-error");
                    downloadBtn.disabled = false;
                };
                
                return currentAudioUrl;
            } catch (error) {
                console.error("Synthesis error:", error);
                updateStatus(`‚ùå Synthesis failed: ${error.message}`, "status-error");
                alert(`Synthesis failed: ${error.message}\nCheck console for details.`);
                return null;
            } finally {
                synthesizeBtn.disabled = false;
                playExampleBtn.disabled = false;
            }
        }
        
        function downloadWav() {
            if (!currentWavBlob) {
                alert("No audio to download. Please synthesize first.");
                return;
            }
            try {
                const activeTab = document.querySelector('.tab.active').dataset.tab;
                let filename = 'jim_output';
                if (activeTab === 'word') {
                    const text = textInput.value.trim().substring(0, 30).replace(/[^\w\s]/g, '');
                    filename = text ? `jim_${text.replace(/\s+/g, '_')}` : 'jim_output';
                } else if (activeTab === 'phoneme') {
                    filename = 'jim_phonemes';
                } else {
                    filename = 'jim_spec';
                }
                const downloadUrl = URL.createObjectURL(currentWavBlob);
                const link = document.createElement('a');
                link.href = downloadUrl;
                link.download = `${filename}_${currentSampleRate}Hz.wav`;
                document.body.appendChild(link);
                link.click();
                document.body.removeChild(link);
                setTimeout(() => URL.revokeObjectURL(downloadUrl), 1000);
                updateStatus("üíæ Download started!", "status-ready");
            } catch (error) {
                console.error("Download error:", error);
                alert(`Download failed: ${error.message}`);
            }
        }
        
        // Playback controls
        playBtn.addEventListener('click', () => {
            if (audioPlayer.src) {
                audioPlayer.play().catch(e => {
                    console.log("Autoplay prevented:", e);
                    alert("Browser blocked autoplay. Click directly on the audio player controls to play.");
                });
            }
        });
        
        pauseBtn.addEventListener('click', () => {
            audioPlayer.pause();
        });
        
        // Synthesize button - STRICT MODE SELECTION
        synthesizeBtn.addEventListener('click', async () => {
            const activeTab = document.querySelector('.tab.active').dataset.tab;
            let text, mode;
            if (activeTab === 'word') {
                text = textInput.value.trim();
                mode = 'word';
            } else if (activeTab === 'phoneme') {
                text = phonemeInput.value.trim();
                mode = 'phoneme';
            } else {
                text = specInput.value.trim();
                mode = 'spec';
            }
            if (!text) {
                alert("Please enter some input first!");
                return;
            }
            await synthesize(text, mode);
        });
        
        // Example button
        playExampleBtn.addEventListener('click', async () => {
            const activeTab = document.querySelector('.tab.active').dataset.tab;
            if (activeTab === 'word') {
                textInput.value = "hello world this is a robot voice";
                await synthesize(textInput.value, 'word');
            } else if (activeTab === 'phoneme') {
                phonemeInput.value = "HH EH L OW SIL W ER L D SIL DH IH S SIL IH Z SIL AH SIL R OW B AA T SIL V AO Y S";
                await synthesize(phonemeInput.value, 'phoneme');
            } else {
                specInput.value = `# Song
SIL    0.300  0.010  0.0
AH     1.800  0.450  130.8 130.8 146.8 146.8 174.6 174.6 196.0 196.0
AH     1.600  0.420  220.0 220.0 261.6 261.6 233.1 233.1 220.0 220.0
SIL    0.150  0.010  0.0
S      0.140  0.025  220.0
IH     0.220  0.045  220.0 233.1 246.9
N      0.160  0.030  246.9 261.6
TH     0.180  0.035  261.6 277.2
AH     0.240  0.050  277.2 293.7 311.1 311.1
S      0.150  0.028  311.1 293.7
AH     0.260  0.055  293.7 277.2 261.6 261.6
Z      0.170  0.032  261.6 246.9
ER     0.320  0.070  246.9 233.1 220.0 220.0 207.7 207.7
SIL    0.120  0.015  0.0
V      0.150  0.030  207.7
AO     0.240  0.050  207.7 220.0 233.1 233.1
Y      0.180  0.038  233.1 246.9
S      0.160  0.030  246.9 261.6
SIL    0.200  0.020  0.0
AH     1.400  0.400  196.0 196.0 220.0 220.0 261.6 261.6 293.7 293.7
AH     1.500  0.430  311.1 311.1 293.7 293.7 261.6 261.6 233.1 233.1
SIL    0.100  0.012  0.0
S      0.130  0.025  233.1
IH     0.210  0.045  233.1 246.9 261.6
NG     0.230  0.050  261.6 277.2 293.7 293.7
Z      0.140  0.028  293.7 277.2
SIL    0.110  0.018  0.0
D      0.120  0.025  277.2
IH     0.200  0.042  277.2 293.7 311.1
JH     0.150  0.030  311.1 329.6
AH     0.220  0.048  329.6 311.1 293.7 293.7
T      0.130  0.025  293.7 277.2
AH     0.240  0.052  277.2 261.6 246.9 246.9
L      0.170  0.035  246.9 233.1
SIL    0.140  0.020  0.0
D      0.120  0.025  233.1
R      0.160  0.032  233.1 220.0
IY     0.280  0.060  220.0 233.1 246.9 261.6 277.2 277.2
M      0.180  0.038  277.2 261.6
Z      0.210  0.045  261.6 246.9 233.1 233.1
SIL    0.180  0.025  0.0
AH     1.600  0.460  220.0 220.0 196.0 196.0 174.6 174.6 155.6 155.6
ER     1.700  0.480  155.6 164.8 174.6 185.0 196.0 207.7 220.0 233.1
SIL    0.120  0.015  0.0
AH     0.200  0.042  233.1 246.9 261.6
K      0.140  0.028  261.6 246.9
R      0.170  0.035  246.9 233.1
AH     0.220  0.048  233.1 220.0 207.7 207.7
S      0.150  0.030  207.7 196.0
SIL    0.100  0.018  0.0
DH     0.130  0.026  196.0
AH     0.240  0.052  196.0 207.7 220.0 233.1
W      0.160  0.033  233.1 246.9
AY     0.260  0.058  246.9 261.6 277.2 293.7 311.1 311.1
R      0.180  0.038  311.1 293.7
SIL    0.150  0.022  0.0
T      0.120  0.025  293.7
AH     0.210  0.045  293.7 277.2 261.6
N      0.190  0.040  261.6 246.9
AY     0.270  0.062  246.9 261.6 277.2 293.7 311.1 329.6 349.2 349.2
T      0.160  0.033  349.2 329.6
SIL    0.400  0.030  0.0
AH     2.200  0.650  196.0 185.0 174.6 164.8 155.6 146.8 138.6 130.8
ER     2.400  0.700  130.8 138.6 146.8 155.6 164.8 174.6 185.0 196.0
SIL    0.800  0.010  0.0`;
                await synthesize(specInput.value, 'spec');
            }
        });
        
        // Download button
        downloadBtn.addEventListener('click', downloadWav);
        
        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            if (currentAudioUrl) {
                URL.revokeObjectURL(currentAudioUrl);
            }
        });
        
        // Initialize
        window.addEventListener('load', setupPyodide);
    </script>
</body>
</html>
