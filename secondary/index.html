<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FSB4 Speech Synthesizer (Pyodide)</title>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 28px;
            margin-bottom: 5px;
        }
        
        .header p {
            opacity: 0.9;
            font-size: 14px;
        }
        
        .tabs {
            display: flex;
            background: #f0f0f0;
            border-bottom: 2px solid #e0e0e0;
        }
        
        .tab {
            padding: 15px 25px;
            cursor: pointer;
            font-weight: 600;
            color: #666;
            border-bottom: 3px solid transparent;
            transition: all 0.3s;
        }
        
        .tab:hover {
            background: #e8e8e8;
        }
        
        .tab.active {
            color: #667eea;
            border-bottom: 3px solid #667eea;
            background: white;
        }
        
        .tab-content {
            padding: 20px;
            display: none;
        }
        
        .tab-content.active {
            display: block;
        }
        
        .split-pane {
            display: grid;
            grid-template-columns: 300px 1fr;
            gap: 20px;
            height: 400px;
        }
        
        .phoneme-list {
            border: 2px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            overflow-y: auto;
            background: #f9f9f9;
        }
        
        .phoneme-list div {
            padding: 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background 0.2s;
        }
        
        .phoneme-list div:hover {
            background: #e0e0ff;
        }
        
        .editor-container {
            display: flex;
            flex-direction: column;
            height: 100%;
        }
        
        .editor-label {
            font-weight: 600;
            margin-bottom: 8px;
            color: #333;
        }
        
        textarea {
            flex: 1;
            padding: 12px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            resize: none;
            background: #f9f9f9;
        }
        
        textarea:focus {
            outline: none;
            border-color: #667eea;
        }
        
        .button-group {
            display: flex;
            gap: 10px;
            margin-top: 15px;
            flex-wrap: wrap;
        }
        
        button {
            padding: 10px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }
        
        button:active {
            transform: translateY(0);
        }
        
        button.secondary {
            background: #6c757d;
        }
        
        button.success {
            background: #28a745;
        }
        
        button.danger {
            background: #dc3545;
        }
        
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }
        
        .control-group {
            margin-bottom: 20px;
        }
        
        .control-group label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: #333;
        }
        
        .control-row {
            display: flex;
            align-items: center;
            gap: 15px;
            margin-bottom: 15px;
        }
        
        .control-row label {
            margin-bottom: 0;
            min-width: 150px;
        }
        
        select, input[type="number"] {
            padding: 10px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-size: 14px;
        }
        
        select:focus, input[type="number"]:focus {
            outline: none;
            border-color: #667eea;
        }
        
        input[type="range"] {
            flex: 1;
            height: 8px;
            -webkit-appearance: none;
            background: #ddd;
            border-radius: 4px;
        }
        
        input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #667eea;
            cursor: pointer;
        }
        
        .value-display {
            min-width: 50px;
            text-align: right;
            font-weight: 600;
            color: #667eea;
        }
        
        .status-bar {
            background: #2d2d2d;
            color: #00ff00;
            padding: 12px;
            font-family: 'Courier New', monospace;
            font-size: 13px;
            border-radius: 8px;
            margin-top: 20px;
            overflow-x: auto;
        }
        
        .playback-controls {
            background: #f8f9fa;
            padding: 20px;
            border-top: 2px solid #e0e0e0;
            text-align: center;
        }
        
        .playback-buttons {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-top: 15px;
        }
        
        .reference-text {
            background: #f9f9f9;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 13px;
            line-height: 1.6;
            white-space: pre-wrap;
            max-height: 600px;
            overflow-y: auto;
            border: 2px solid #ddd;
        }
        
        .reference-text strong {
            color: #667eea;
        }
        
        .file-input-wrapper {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        input[type="file"] {
            display: none;
        }
        
        .file-label {
            padding: 10px 20px;
            background: #6c757d;
            color: white;
            border-radius: 8px;
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .file-label:hover {
            background: #5a6268;
        }
        
        .file-name {
            color: #667eea;
            font-weight: 600;
        }
        
        .loading {
            text-align: center;
            padding: 40px;
            color: #667eea;
            font-size: 18px;
        }
        
        .spinner {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #667eea;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        
        .alert {
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 15px;
            font-weight: 600;
        }
        
        .alert-success {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        
        .alert-error {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        
        .alert-info {
            background: #d1ecf1;
            color: #0c5460;
            border: 1px solid #bee5eb;
        }
        
        @media (max-width: 768px) {
            .split-pane {
                grid-template-columns: 1fr;
                height: auto;
            }
            
            .tabs {
                flex-wrap: wrap;
            }
            
            .tab {
                padding: 12px;
                font-size: 13px;
            }
            
            .control-row {
                flex-direction: column;
                align-items: flex-start;
            }
            
            .control-row label {
                margin-bottom: 5px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>FSB4 Speech Synthesizer (Pyodide)</h1>
            <p>Formant-based speech synthesis running in your browser</p>
        </div>
        
        <div id="loading-screen" class="loading">
            <div class="spinner"></div>
            <p>Loading Pyodide and FSB4 engine...</p>
        </div>
        
        <div id="main-app" style="display: none;">
            <div class="tabs">
                <div class="tab active" data-tab="editor">Phoneme Editor</div>
                <div class="tab" data-tab="voice">Voice Controls</div>
                <div class="tab" data-tab="render">Audio Rendering</div>
                <div class="tab" data-tab="reference">Reference</div>
            </div>
            
            <div class="tab-content active" id="editor-tab">
                <div class="split-pane">
                    <div>
                        <div class="editor-label">Phoneme Library (click to insert)</div>
                        <div class="phoneme-list" id="phoneme-list"></div>
                    </div>
                    <div class="editor-container">
                        <div class="editor-label">Phoneme Spec (PHONEME  DUR    OVRLP  P0 [P1...]):</div>
                        <textarea id="spec-editor" spellcheck="false"></textarea>
                        <div class="button-group">
                            <button onclick="loadSpecFile()">Load Spec</button>
                            <button onclick="saveSpecFile()">Save Spec</button>
                            <button onclick="clearEditor()">Clear</button>
                            <button onclick="parseSpec()">Parse to Phonemes</button>
                            <button onclick="saveBytecode()">→ Save Bytecode (.phx)</button>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="tab-content" id="voice-tab">
                <div class="control-group">
                    <div class="control-row">
                        <label>Active Voice:</label>
                        <select id="voice-select"></select>
                        <label style="margin-left: 30px;">Pitch Base:</label>
                        <input type="number" id="pitch-base" value="115" min="50" max="400" style="width: 80px;">
                        <span>Hz</span>
                    </div>
                </div>
                
                <div class="control-group">
                    <div class="control-row">
                        <label>Speed (%):</label>
                        <input type="range" id="speed-slider" min="50" max="200" value="100">
                        <span class="value-display" id="speed-value">100</span>
                    </div>
                    <div class="control-row">
                        <label>Formant Shift (Hz):</label>
                        <input type="range" id="formant-slider" min="-200" max="200" value="0">
                        <span class="value-display" id="formant-value">0</span>
                    </div>
                </div>
                
                <div class="control-group">
                    <div class="button-group">
                        <button onclick="loadPHXFile()">Load .PHX Bytecode</button>
                        <button onclick="savePHXFile()">Save .PHX Bytecode</button>
                        <button onclick="loadPHNFile()">Load .PHN (Legacy)</button>
                        <button onclick="exportWAV()">Export Rendered WAV</button>
                    </div>
                </div>
            </div>
            
            <div class="tab-content" id="render-tab">
                <div class="alert alert-info">
                    <strong>RENDER WORKFLOW:</strong><br>
                    1. Click "Render Audio from .phx File" below<br>
                    2. SELECT a valid .phx bytecode file<br>
                    3. FSB4 will LOAD the bytecode → SYNTHESIZE audio → CACHE buffer<br>
                    4. Use playback controls to hear the cached audio (ZERO synthesis overhead)<br><br>
                    <strong>OVRLP FORMAT:</strong> Each phoneme now includes overlap duration (seconds) for smooth crossfading
                </div>
                
                <div style="text-align: center; margin-top: 30px;">
                    <button onclick="renderAudioFromFile()" style="padding: 15px 40px; font-size: 16px;">
                        Render Audio from .phx File
                    </button>
                </div>
            </div>
            
            <div class="tab-content" id="reference-tab">
                <div class="reference-text">
PHONEME REFERENCE GUIDE (OVRLP FORMAT)
═══════════════════════════════════════════════════════════════════════════════
NEW FORMAT:
PHONEME  DUR    OVRLP  P0 [P1 P2 ...]
• DUR   = Duration in seconds (0.01-2.0s)
• OVRLP = Overlap duration into NEXT phoneme (0.0-0.5s)
→ Enables smooth crossfading between phonemes
→ Typical values: vowels 0.018-0.030s, consonants 0.008-0.015s

VOWELS:      AH AE AA AO EH EY IH IY OW UH UW ER
STOPS:       P T K B D G CH (unvoiced/voiced pairs)
FRICATIVES:  F S SH TH (unvoiced)  V Z ZH DH (voiced)
NASALS:      M N NG
LIQUIDS:     L R
GLIDES:      W Y HH JH

EXAMPLE WORDS WITH OVRLP:
hello    → HH 0.080 0.012 115.0
          EH 0.120 0.025 115.0
          L  0.110 0.015 115.0
          AO 0.140 0.030 112.0
          OW 0.180 0.000 105.0

SPECIAL NOTES:
• SIL = silence (0.19s default, OVRLP=0.0)
• _FINAL suffix increases vowel duration by 40%
• Pitch contours: space-separated Hz values (max 8 points)
• OVRLP=0.0 means NO blending with next phoneme (use for final phonemes)

TECHNICAL SPECS:
• Sample rate: 48 kHz
• Format: .PHX (54 bytes/phoneme with OVRLP) - parameterized bytecode
• Legacy: .PHN (1 byte/phoneme) - simple phoneme stream
• Formant synthesis with glottal pulse modeling + crossfade blending

FSB4 ARCHITECTURE ENFORCEMENT:
✓ Spec → Parse → Internal specs WITH OVERLAP (NO audio)
✓ Specs → Save Bytecode (.phx) = FILE I/O ONLY (NO audio)
✓ Render Audio = FILE SELECTOR → LOAD .phx → SYNTHESIZE → CACHE BUFFER
✓ Play = CACHED BUFFER ONLY (ZERO synthesis during playback)

DEBUG WORKFLOW:
1. Edit spec in Phoneme Editor tab (include OVRLP values!)
2. Click "Parse to Phonemes" to validate
3. Click "→ Save Bytecode (.phx)" to generate VALID bytecode WITH OVERLAP
4. Go to "Audio Rendering" tab → Click "Render Audio from .phx File"
5. SELECT .phx file → Audio synthesized WITH crossfade blending → cached
6. Use playback controls to hear cached audio (NO synthesis overhead)

EXAMPLE PHONEME INPUT (FULL OVRLP FORMAT):
SIL  0.190 0.000 0.0
HH   0.080 0.012 115.0
EH   0.120 0.025 115.0 118.0
L    0.110 0.015 115.0
AO   0.140 0.030 112.0 108.0
OW   0.180 0.000 105.0 100.0
SIL  0.280 0.000 0.0
                </div>
            </div>
            
            <div class="playback-controls">
                <div class="editor-label">Playback Controls (cached audio ONLY - NO SYNTHESIS)</div>
                <div class="playback-buttons">
                    <button id="play-btn" onclick="playCachedAudio()" disabled>▶ Play Cached Audio</button>
                    <button id="stop-btn" onclick="stopPlayback()" disabled>■ Stop</button>
                </div>
            </div>
            
            <div class="status-bar" id="status-bar">
                Workflow: Parse → Save Bytecode → Render from .phx File → Play Cached Buffer
            </div>
        </div>
    </div>

    <script>
        // Global state
        let pyodide = null;
        let fsbModule = null;
        let currentSpecs = [];
        let renderedAudio = null;
        let audioContext = null;
        let audioBuffer = null;
        let audioSource = null;
        let isPlaying = false;
        
        // Initialize Pyodide
        async function loadPyodideAndFSB4() {
            try {
                // Load Pyodide
                pyodide = await loadPyodide();
                
                // Install required packages
                await pyodide.loadPackage(['numpy', 'scipy']);
                
                // Load FSB4.py
                const fsb4Code = `
${document.getElementById('fsb4-code').textContent}
`;
                pyodide.runPython(fsb4Code);
                
                // Get FSB4 module reference
                fsbModule = pyodide.globals.get('fsb');
                
                // Initialize
                initializeUI();
                
                document.getElementById('loading-screen').style.display = 'none';
                document.getElementById('main-app').style.display = 'block';
                
                updateStatus('✓ FSB4 engine loaded successfully!');
            } catch (error) {
                console.error('Error loading Pyodide:', error);
                document.getElementById('loading-screen').innerHTML = 
                    '<p style="color: #dc3545;">Error loading FSB4 engine. Please refresh the page.</p>';
            }
        }
        
        function initializeUI() {
            // Setup tab switching
            document.querySelectorAll('.tab').forEach(tab => {
                tab.addEventListener('click', () => {
                    document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
                    tab.classList.add('active');
                    document.getElementById(tab.dataset.tab + '-tab').classList.add('active');
                });
            });
            
            // Setup sliders
            document.getElementById('speed-slider').addEventListener('input', (e) => {
                document.getElementById('speed-value').textContent = e.target.value;
            });
            
            document.getElementById('formant-slider').addEventListener('input', (e) => {
                document.getElementById('formant-value').textContent = e.target.value;
            });
            
            // Load phoneme library
            loadPhonemeLibrary();
            
            // Load voices
            loadVoices();
        }
        
        function loadPhonemeLibrary() {
            const phonemeList = document.getElementById('phoneme-list');
            const byteToPhoneme = fsbModule.BYTE_TO_PHONEME.toJs();
            
            // Sort by byte value
            const sortedEntries = Object.entries(byteToPhoneme).sort((a, b) => parseInt(a[0]) - parseInt(b[0]));
            
            phonemeList.innerHTML = '';
            sortedEntries.forEach(([byteVal, phoneme]) => {
                const div = document.createElement('div');
                div.textContent = `0x${parseInt(byteVal).toString(16).padStart(2, '0').toUpperCase()} ${phoneme}`;
                div.addEventListener('click', () => addPhonemeToEditor(phoneme));
                phonemeList.appendChild(div);
            });
        }
        
        function addPhonemeToEditor(phoneme) {
            const vowels = fsbModule.VOWELS.toJs();
            const defaultDur = vowels.includes(phoneme) ? '0.140' : '0.120';
            const defaultOverlap = vowels.includes(phoneme) ? '0.025' : '0.010';
            const defaultPitch = '115.0';
            
            const editor = document.getElementById('spec-editor');
            const current = editor.value.trim();
            
            if (current) {
                editor.value += `\\n${phoneme.padEnd(4)} ${defaultDur} ${defaultOverlap} ${defaultPitch}`;
            } else {
                editor.value = `${phoneme.padEnd(4)} ${defaultDur} ${defaultOverlap} ${defaultPitch}`;
            }
            
            renderedAudio = null;
            updatePlayButtonState();
        }
        
        function loadVoices() {
            const voiceRegistry = fsbModule.VOICE_REGISTRY;
            const voices = voiceRegistry.list_voices().toJs();
            const voiceSelect = document.getElementById('voice-select');
            
            voiceSelect.innerHTML = '';
            Object.keys(voices).forEach(voiceName => {
                const option = document.createElement('option');
                option.value = voiceName;
                option.textContent = voiceName;
                voiceSelect.appendChild(option);
            });
            
            // Set default voice
            voiceSelect.value = 'Default';
            voiceSelect.addEventListener('change', (e) => {
                voiceRegistry.set_current_voice(e.target.value);
                updateStatus(`Voice changed to: ${e.target.value}`);
                renderedAudio = null;
                updatePlayButtonState();
            });
        }
        
        function parseSpec() {
            const text = document.getElementById('spec-editor').value.trim();
            if (!text) {
                updateStatus('ERROR: No spec data!');
                return;
            }
            
            try {
                let specs;
                const hasNumbers = /\\d+\\.\\d+/.test(text);
                
                if (!hasNumbers) {
                    // Treat as English text
                    const phonemes = fsbModule.text_to_phonemes(text);
                    const pitch = parseFloat(document.getElementById('pitch-base').value);
                    specs = fsbModule.phonemes_to_spec(phonemes, fsbModule.VOICE_REGISTRY.current_voice, pitch);
                } else {
                    // Parse as phoneme spec
                    specs = fsbModule.parse_phoneme_spec(text, fsbModule.VOICE_REGISTRY.current_voice);
                }
                
                if (!specs || specs.length <= 2) {
                    updateStatus('ERROR: No valid phonemes generated!');
                    return;
                }
                
                currentSpecs = specs.toJs();
                
                // Update editor with normalized format
                const readable = fsbModule.specs_to_readable(specs);
                document.getElementById('spec-editor').value = readable;
                
                const totalDur = currentSpecs.reduce((sum, s) => sum + s.duration, 0);
                updateStatus(`✓ Parsed ${currentSpecs.length - 2} phonemes (${totalDur.toFixed(2)}s total, WITH OVERLAP). Save as .phx bytecode to render audio.`);
            } catch (error) {
                console.error('Parse error:', error);
                updateStatus(`ERROR parsing: ${error.message}`);
            }
        }
        
        function saveBytecode() {
            if (currentSpecs.length === 0) {
                alert('Parse spec to phonemes first!');
                return;
            }
            
            const filename = prompt('Enter filename for .phx bytecode:', 'output.phx');
            if (!filename) return;
            
            try {
                // Create temporary file in Pyodide filesystem
                const specsProxy = pyodide.toPy(currentSpecs);
                fsbModule.save_parameterized_phonemes(filename, specsProxy);
                
                // Read the file and trigger download
                const fileContent = pyodide.FS.readFile(filename);
                downloadFile(filename, fileContent);
                
                updateStatus(`✓ Bytecode SAVED to: ${filename} (WITH OVERLAP DATA - FILE I/O ONLY)`);
            } catch (error) {
                console.error('Save error:', error);
                updateStatus(`ERROR saving PHX: ${error.message}`);
                alert(`Failed to save PHX file:\\n${error.message}`);
            }
        }
        
        function loadPHXFile() {
            const input = document.createElement('input');
            input.type = 'file';
            input.accept = '.phx';
            input.onchange = (e) => {
                const file = e.target.files[0];
                if (!file) return;
                
                const reader = new FileReader();
                reader.onload = async (event) => {
                    try {
                        // Write file to Pyodide filesystem
                        const arrayBuffer = event.target.result;
                        const filename = file.name;
                        pyodide.FS.writeFile(filename, new Uint8Array(arrayBuffer));
                        
                        // Load bytecode
                        const specs = fsbModule.load_parameterized_phonemes(filename);
                        currentSpecs = specs.toJs();
                        
                        // Update editor
                        const readable = fsbModule.specs_to_readable(specs);
                        document.getElementById('spec-editor').value = readable;
                        
                        const totalDur = currentSpecs.reduce((sum, s) => sum + s.duration, 0);
                        updateStatus(`✓ Bytecode LOADED from: ${filename} (${currentSpecs.length} phonemes, ${totalDur.toFixed(2)}s, WITH OVERLAP) - FILE I/O ONLY`);
                    } catch (error) {
                        console.error('Load error:', error);
                        updateStatus(`ERROR loading PHX: ${error.message}`);
                        alert(`Failed to load PHX file:\\n${error.message}`);
                    }
                };
                reader.readAsArrayBuffer(file);
            };
            input.click();
        }
        
        async function renderAudioFromFile() {
            const input = document.createElement('input');
            input.type = 'file';
            input.accept = '.phx';
            input.onchange = async (e) => {
                const file = e.target.files[0];
                if (!file) {
                    updateStatus('Render cancelled - no file selected');
                    return;
                }
                
                try {
                    updateStatus(`.Loading bytecode from: ${file.name}`);
                    
                    // Write file to Pyodide filesystem
                    const arrayBuffer = await file.arrayBuffer();
                    pyodide.FS.writeFile(file.name, new Uint8Array(arrayBuffer));
                    
                    // Load bytecode
                    const specs = fsbModule.load_parameterized_phonemes(file.name);
                    currentSpecs = specs.toJs();
                    
                    updateStatus('.Synthesizing audio with crossfade blending...');
                    
                    // Synthesize audio
                    const synth = fsbModule.FormantSynthesizer(
                        fsbModule.VOICE_REGISTRY.current_voice,
                        fsbModule.smp
                    );
                    const audioBufferPy = synth.synthesize_from_specs(specs);
                    
                    // Convert to JavaScript array
                    const audioArray = audioBufferPy.toJs();
                    let audioData = Float32Array.from(audioArray);
                    
                    // Apply speed adjustment
                    const speedFactor = document.getElementById('speed-slider').value / 100.0;
                    if (speedFactor !== 1.0) {
                        const originalLength = audioData.length;
                        const newLength = Math.floor(originalLength / speedFactor);
                        const xOld = Array.from({length: originalLength}, (_, i) => i / (originalLength - 1));
                        const xNew = Array.from({length: newLength}, (_, i) => i / (newLength - 1));
                        
                        const newData = new Float32Array(newLength);
                        for (let i = 0; i < newLength; i++) {
                            const x = xNew[i];
                            const idx = x * (originalLength - 1);
                            const floorIdx = Math.floor(idx);
                            const frac = idx - floorIdx;
                            
                            if (floorIdx < originalLength - 1) {
                                newData[i] = audioData[floorIdx] * (1 - frac) + audioData[floorIdx + 1] * frac;
                            } else {
                                newData[i] = audioData[originalLength - 1];
                            }
                        }
                        audioData = newData;
                    }
                    
                    // Cache the rendered audio
                    renderedAudio = audioData;
                    const duration = audioData.length / fsbModule.smp;
                    
                    updateStatus(`✓ Audio RENDERED with crossfade blending (${duration.toFixed(2)}s). Click PLAY to hear cached buffer.`);
                    updatePlayButtonState();
                } catch (error) {
                    console.error('Render error:', error);
                    renderedAudio = null;
                    updateStatus(`Render error: ${error.message}`);
                    alert(`Failed to render audio from bytecode:\\n${error.message}`);
                }
            };
            input.click();
        }
        
        function playCachedAudio() {
            if (!renderedAudio) {
                alert('No Rendered Audio\\n\\nRender audio first using "Render Audio from .phx File" button!\\n\\nWorkflow:\\n1. Save or obtain a valid .phx bytecode file WITH OVERLAP DATA\\n2. Click "Render Audio from .phx File" in Audio Rendering tab\\n3. SELECT the .phx file in the file dialog\\n4. THEN click PLAY');
                updateStatus('ERROR: Render audio before playback');
                return;
            }
            
            if (isPlaying) {
                stopPlayback();
            }
            
            try {
                // Setup Web Audio API
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }
                
                // Create audio buffer
                const buffer = audioContext.createBuffer(
                    1,
                    renderedAudio.length,
                    fsbModule.smp
                );
                
                const channelData = buffer.getChannelData(0);
                for (let i = 0; i < renderedAudio.length; i++) {
                    channelData[i] = renderedAudio[i];
                }
                
                // Play audio
                audioSource = audioContext.createBufferSource();
                audioSource.buffer = buffer;
                audioSource.connect(audioContext.destination);
                audioSource.start(0);
                
                // Update UI
                isPlaying = true;
                updatePlayButtonState();
                updateStatus('Playing CACHED audio buffer with crossfade blending (ZERO synthesis overhead)...');
                
                // Listen for end of playback
                audioSource.onended = () => {
                    isPlaying = false;
                    updatePlayButtonState();
                    updateStatus('Playback complete (cached buffer with crossfade blending)');
                };
            } catch (error) {
                console.error('Playback error:', error);
                updateStatus(`Playback error: ${error.message}`);
            }
        }
        
        function stopPlayback() {
            if (audioSource) {
                try {
                    audioSource.stop();
                } catch (e) {
                    // Ignore errors if already stopped
                }
            }
            
            isPlaying = false;
            updatePlayButtonState();
            updateStatus('Playback stopped');
        }
        
        function updatePlayButtonState() {
            const playBtn = document.getElementById('play-btn');
            const stopBtn = document.getElementById('stop-btn');
            
            playBtn.disabled = !(renderedAudio && !isPlaying);
            stopBtn.disabled = !isPlaying;
        }
        
        function updateStatus(message) {
            document.getElementById('status-bar').textContent = message;
        }
        
        function downloadFile(filename, content) {
            const blob = new Blob([content], { type: 'application/octet-stream' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = filename;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }
        
        function loadSpecFile() {
            const input = document.createElement('input');
            input.type = 'file';
            input.accept = '.txt';
            input.onchange = (e) => {
                const file = e.target.files[0];
                if (!file) return;
                
                const reader = new FileReader();
                reader.onload = (event) => {
                    document.getElementById('spec-editor').value = event.target.result;
                    renderedAudio = null;
                    updatePlayButtonState();
                    updateStatus(`Loaded spec from: ${file.name}`);
                };
                reader.readAsText(file);
            };
            input.click();
        }
        
        function saveSpecFile() {
            const content = document.getElementById('spec-editor').value;
            const filename = prompt('Enter filename:', 'spec.txt') || 'spec.txt';
            
            const blob = new Blob([content], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = filename;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
            
            updateStatus(`Saved spec to: ${filename}`);
        }
        
        function clearEditor() {
            document.getElementById('spec-editor').value = '';
            renderedAudio = null;
            updatePlayButtonState();
        }
        
        function exportWAV() {
            if (!renderedAudio) {
                alert('Render First\\n\\nRender audio before exporting WAV!');
                return;
            }
            
            const filename = prompt('Enter WAV filename:', 'output.wav') || 'output.wav';
            
            try {
                // Convert to WAV format
                const audioData = renderedAudio;
                const sampleRate = fsbModule.smp;
                
                // Create WAV file in Pyodide
                pyodide.runPython(`
import numpy as np
import wave

audio_data = np.array(${JSON.stringify(Array.from(audioData))}, dtype=np.float32)
filename = "${filename}"

# Clip and convert to int16
audio_clipped = np.clip(audio_data * 32767, -32768, 32767).astype(np.int16)

with wave.open(filename, 'wb') as wf:
    wf.setnchannels(1)
    wf.setsampwidth(2)
    wf.setframerate(${sampleRate})
    wf.writeframes(audio_clipped.tobytes())
`);
                
                // Read and download
                const wavContent = pyodide.FS.readFile(filename);
                downloadFile(filename, wavContent);
                
                const sizeKB = wavContent.length / 1024;
                const duration = audioData.length / sampleRate;
                updateStatus(`✓ WAV exported: ${filename} (${sizeKB.toFixed(1)} KB, ${duration.toFixed(2)}s with crossfade)`);
            } catch (error) {
                console.error('Export error:', error);
                updateStatus(`ERROR exporting WAV: ${error.message}`);
                alert(`Failed to export WAV file:\\n${error.message}`);
            }
        }
        
        function loadPHNFile() {
            const input = document.createElement('input');
            input.type = 'file';
            input.accept = '.phn';
            input.onchange = async (e) => {
                const file = e.target.files[0];
                if (!file) return;
                
                try {
                    const arrayBuffer = await file.arrayBuffer();
                    const bytes = new Uint8Array(arrayBuffer);
                    
                    // Write to Pyodide filesystem
                    pyodide.FS.writeFile(file.name, bytes);
                    
                    // Load and convert
                    const content = pyodide.FS.readFile(file.name);
                    const byteData = new Uint8Array(content);
                    
                    // Parse PHN file (simplified)
                    const phonemes = [];
                    const byteToPhoneme = fsbModule.BYTE_TO_PHONEME.toJs();
                    
                    for (let i = 0; i < byteData.length; i++) {
                        const byteVal = byteData[i];
                        if (byteVal in byteToPhoneme) {
                            phonemes.push(byteToPhoneme[byteVal]);
                        }
                    }
                    
                    if (phonemes.length === 0) {
                        updateStatus('ERROR: No valid phonemes in PHN file!');
                        return;
                    }
                    
                    // Convert to specs with overlap
                    const specs = [];
                    const vowels = fsbModule.VOWELS.toJs();
                    
                    for (let i = 0; i < phonemes.length; i++) {
                        const ph = phonemes[i];
                        const phData = fsbModule.VOICE_REGISTRY.current_voice.get_phoneme_data(ph);
                        const duration = phData.get('length', 0.14);
                        const overlap = (vowels.includes(ph) && i < phonemes.length - 1) ? 0.018 : 0.008;
                        const pitch = (phData.get('voiced', false) && ph !== 'SIL') ? 115.0 : 0.0;
                        
                        specs.push({
                            'phoneme': ph,
                            'duration': duration,
                            'overlap': overlap,
                            'pitch_contour': [pitch],
                            'num_pitch_points': 1,
                            'f1': phData.get('f1', 0.0) || 0.0,
                            'f2': phData.get('f2', 0.0) || 0.0,
                            'f3': phData.get('f3', 0.0) || 0.0,
                            'voiced': !['SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'].includes(ph)
                        });
                    }
                    
                    currentSpecs = specs;
                    renderedAudio = null;
                    updatePlayButtonState();
                    
                    // Update editor
                    const specsProxy = pyodide.toPy(specs);
                    const readable = fsbModule.specs_to_readable(specsProxy);
                    document.getElementById('spec-editor').value = readable;
                    
                    updateStatus(`Loaded ${phonemes.length} legacy phonemes → CONVERTED TO OVRLP FORMAT (NOT valid .phx bytecode)`);
                } catch (error) {
                    console.error('PHN load error:', error);
                    updateStatus(`ERROR loading PHN: ${error.message}`);
                }
            };
            input.click();
        }
        
        // Start loading
        loadPyodideAndFSB4();
    </script>
    
    <!-- Hidden element containing FSB4.py code -->
    <div id="fsb4-code" style="display: none;">
import numpy as np
import scipy.signal as sig
import wave
import re
import os
import json
import subprocess
from pathlib import Path
from typing import Dict, List

smp = 48000
VOICES_DIR = Path("voices")
VOICES_DIR.mkdir(exist_ok=True)

BYTE_TO_PHONEME = {
    0x00: 'SIL', 0x01: 'AH', 0x02: 'AE', 0x03: 'AA', 0x04: 'AO', 0x05: 'EH', 0x06: 'EY',
    0x07: 'IH', 0x08: 'IY', 0x09: 'OW', 0x0A: 'UH', 0x0B: 'UW', 0x0C: 'ER', 0x0D: 'B',
    0x0E: 'D', 0x0F: 'G', 0x10: 'P', 0x11: 'T', 0x12: 'K', 0x13: 'M', 0x14: 'N', 0x15: 'NG',
    0x16: 'L', 0x17: 'R', 0x18: 'F', 0x19: 'S', 0x1A: 'SH', 0x1B: 'TH', 0x1C: 'DH', 0x1D: 'V',
    0x1E: 'Z', 0x1F: 'ZH', 0x20: 'W', 0x21: 'Y', 0x22: 'HH', 0x23: 'CH', 0x24: 'JH',
}
PHONEME_TO_BYTE = {v: k for k, v in BYTE_TO_PHONEME.items()}

VOWELS = {'AH','AE','AA','AO','EH','EY','IH','IY','OW','UH','UW','ER'}
STOPS = {'P','T','K','B','D','G','CH'}
FRICATIVES_UNVOICED = {'F','S','SH','TH','HH'}
FRICATIVES_VOICED = {'V','Z','ZH','DH'}

class Voice:
    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self.phonemes: Dict[str, Dict] = {}
    
    def get_phoneme_data(self, phoneme: str) -> Dict:
        if phoneme.endswith('_FINAL'):
            base_ph = phoneme.replace('_FINAL', '')
            data = self.phonemes.get(base_ph, self.phonemes.get('SIL', {})).copy()
            if base_ph in VOWELS:
                data['length'] = min(data.get('length', 0.14) * 1.4, 0.35)
            return data
        return self.phonemes.get(phoneme, self.phonemes.get('SIL', {}))
    
    def save(self, filepath: str) -> None:
        data = {"name": self.name, "description": self.description, "phonemes": self.phonemes}
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"Voice '{self.name}' saved to: {filepath}")
    
    @classmethod
    def load(cls, filepath: str) -> 'Voice':
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        voice = cls(data['name'], data.get('description', ''))
        voice.phonemes = data['phonemes']
        return voice

class DefaultVoice(Voice):
    def __init__(self):
        super().__init__("Default", "Built-in robotic voice")
        self.phonemes = {
            'AH': {'f1': 700, 'f2': 1100, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AE': {'f1': 650, 'f2': 1250, 'f3': 2500, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AA': {'f1': 620, 'f2': 1180, 'f3': 2550, 'f4': 115, 'length': 0.14, 'voiced': True},
            'AO': {'f1': 550, 'f2': 850, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'EH': {'f1': 530, 'f2': 1700, 'f3': 2450, 'f4': 115, 'length': 0.14, 'voiced': True},
            'EY': {'f1': 400, 'f2': 2100, 'f3': 2800, 'f4': 115, 'length': 0.14, 'voiced': True},
            'IH': {'f1': 420, 'f2': 1950, 'f3': 2500, 'f4': 115, 'length': 0.14, 'voiced': True},
            'IY': {'f1': 300, 'f2': 2250, 'f3': 3000, 'f4': 115, 'length': 0.14, 'voiced': True},
            'OW': {'f1': 450, 'f2': 900, 'f3': 2350, 'f4': 115, 'length': 0.14, 'voiced': True},
            'UH': {'f1': 400, 'f2': 650, 'f3': 2400, 'f4': 115, 'length': 0.14, 'voiced': True},
            'UW': {'f1': 330, 'f2': 900, 'f3': 2200, 'f4': 115, 'length': 0.14, 'voiced': True},
            'ER': {'f1': 480, 'f2': 1180, 'f3': 1650, 'f4': 115, 'length': 0.14, 'voiced': True},
            'M':  {'f1': 350, 'f2': 1050, 'f3': 2250, 'f4': 115, 'length': 0.12, 'voiced': True},
            'N':  {'f1': 320, 'f2': 1150, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'NG': {'f1': 280, 'f2': 950, 'f3': 2350, 'f4': 115, 'length': 0.12, 'voiced': True},
            'L':  {'f1': 400, 'f2': 1150, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'R':  {'f1': 450, 'f2': 1250, 'f3': 1500, 'f4': 115, 'length': 0.12, 'voiced': True},
            'DH': {'f1': 380, 'f2': 1650, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'V':  {'f1': 380, 'f2': 1550, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'Z':  {'f1': 380, 'f2': 1750, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'ZH': {'f1': 380, 'f2': 1450, 'f3': 2250, 'f4': 115, 'length': 0.12, 'voiced': True},
            'W':  {'f1': 350, 'f2': 700, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'Y':  {'f1': 350, 'f2': 2050, 'f3': 2650, 'f4': 115, 'length': 0.12, 'voiced': True},
            'JH': {'f1': 400, 'f2': 1650, 'f3': 2450, 'f4': 115, 'length': 0.12, 'voiced': True},
            'B':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'D':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'G':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'P':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'T':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'K':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'F':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'S':  {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'SH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'TH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'HH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.125, 'voiced': False},
            'CH': {'f1': None, 'f2': None, 'f3': None, 'f4': 0, 'length': 0.068, 'voiced': False},
            'SIL': {'f1': 0, 'f2': 0, 'f3': 0, 'f4': 0, 'length': 0.19, 'voiced': 'silence'},
        }

class VoiceRegistry:
    def __init__(self):
        self.voices: Dict[str, Voice] = {'Default': DefaultVoice()}
        self._load_custom_voices()
        self.current_voice: Voice = self.voices['Default']
    
    def _load_custom_voices(self):
        for filepath in VOICES_DIR.glob("*.json"):
            try:
                voice = Voice.load(filepath)
                self.voices[voice.name] = voice
            except Exception:
                pass
    
    def set_current_voice(self, name: str) -> bool:
        if name in self.voices:
            self.current_voice = self.voices[name]
            return True
        return False
    
    def list_voices(self) -> Dict[str, Voice]:
        return self.voices

VOICE_REGISTRY = VoiceRegistry()

WORD_MAP = {
    'hello': ['HH', 'EH', 'L', 'AO', 'OW'], 'world': ['W', 'ER', 'L', 'D'], 'test': ['T', 'EH', 'S', 'T'],
    'one': ['W', 'AH', 'N'], 'two': ['T', 'UW'], 'this': ['DH', 'IH', 'S'], 'is': ['IH', 'S'],
    'text': ['T', 'AE', 'K', 'S', 'T'], 'a': ['AH'], 'three': ['TH', 'R', 'IY'], 'four': ['F', 'AO', 'R'],
    'five': ['F', 'AA', 'EY', 'V'], 'six': ['S', 'IH', 'K', 'S'], 'seven': ['S', 'EH', 'V', 'EH', 'N'],
    'eight': ['EY', 'T'], 'nine': ['N', 'AA', 'EY', 'N'], 'ten': ['T', 'EH', 'N'],
    'robot': ['R', 'OW', 'B', 'AH', 'T'], 'voice': ['V', 'AO', 'Y', 'S'], 'i': ['AA', 'EY'],
    'am': ['AH', 'M'], 'and': ['AH', 'N', 'D'], 'single': ['S', 'IH', 'NG', 'G', 'AH', 'L'],
    'yes': ['Y', 'EH', 'S'], 'no': ['N', 'OW'], 'hi': ['HH', 'AA', 'EY'],
    'formant': ['F', 'AO', 'R', 'M', 'AH', 'N', 'T'], 'speech': ['S', 'P', 'IY', 'CH'],
    'synthesis': ['S', 'IH', 'N', 'TH', 'AH', 'S', 'IH', 'S'], 'tts': ['T', 'IY', 'T', 'IY', 'EH', 'S'],
    'computer': ['K', 'AH', 'M', 'P', 'Y', 'UW', 'T', 'ER'], 'please': ['P', 'L', 'IY', 'Z'],
    'thank': ['TH', 'AE', 'NG', 'K'], 'you': ['Y', 'UW'], 'good': ['G', 'UH', 'D'], 'morning': ['M', 'AO', 'R', 'N', 'IH', 'NG'],
    'afternoon': ['AE', 'F', 'T', 'ER', 'N', 'UW', 'N'], 'evening': ['IY', 'V', 'N', 'IH', 'NG'],
    'ship': ['SH', 'IH', 'P'], 'fish': ['F', 'IH', 'SH'], 'think': ['TH', 'IH', 'NG', 'K'],
    'sushi': ['S', 'UW', 'SH', 'IY'], 'zip': ['Z', 'IH', 'P'], 'measure': ['M', 'EH', 'ZH', 'ER'],
    'thin': ['TH', 'IH', 'N'], 'thick': ['TH', 'IH', 'K'], 'thistle': ['TH', 'IH', 'S', 'AH', 'L'],
}

def text_to_phonemes(text: str) -> List[str]:
    text = text.lower().strip()
    text = re.sub(r'[^a-z\s]', '', text)
    words = text.split()
    phoneme_sequence = ['SIL']
    for i, word in enumerate(words):
        phons = WORD_MAP.get(word, ['SIL'])
        phoneme_sequence.extend(phons)
        if i < len(words) - 1:
            phoneme_sequence.append('SIL')
    phoneme_sequence.append('SIL')
    return phoneme_sequence

def phonemes_to_spec(phonemes: List[str], voice: Voice, pitch_base: float = 115.0) -> List[Dict]:
    specs = []
    for i, ph in enumerate(phonemes):
        ph_data = voice.get_phoneme_data(ph)
        duration = ph_data.get('length', 0.14)
        overlap = 0.018 if ph in VOWELS and i < len(phonemes) - 1 else 0.008
        
        if ph == 'SIL':
            pitch = [0.0]
        elif ph in VOWELS:
            if i == len(phonemes) - 2:
                pitch = [pitch_base * 0.95, pitch_base * 0.90]
            elif i == 1:
                pitch = [pitch_base * 1.05, pitch_base * 1.10]
            else:
                pitch = [pitch_base]
        else:
            pitch = [pitch_base if ph_data.get('voiced', False) else 0.0]
        
        f1 = ph_data.get('f1', 0.0) or 0.0
        f2 = ph_data.get('f2', 0.0) or 0.0
        f3 = ph_data.get('f3', 0.0) or 0.0
        
        specs.append({
            'phoneme': ph,
            'duration': duration,
            'overlap': overlap,
            'pitch_contour': pitch,
            'num_pitch_points': len(pitch),
            'f1': f1,
            'f2': f2,
            'f3': f3,
            'voiced': ph not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
        })
    return specs

def parse_phoneme_spec(text: str, voice: Voice) -> List[Dict]:
    specs = []
    for line_num, line in enumerate(text.splitlines(), 1):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split()
        if len(parts) < 4:
            continue
        ph_name = parts[0].upper()
        if ph_name not in PHONEME_TO_BYTE:
            continue
        try:
            duration = max(0.01, min(2.0, float(parts[1])))
            overlap = max(0.0, min(0.5, float(parts[2])))
            pitch_points = [float(p) for p in parts[3:]]
            if len(pitch_points) > 8:
                pitch_points = pitch_points[:8]
            ph_data = voice.get_phoneme_data(ph_name)
            f1 = ph_data.get('f1', 0.0) or 0.0
            f2 = ph_data.get('f2', 0.0) or 0.0
            f3 = ph_data.get('f3', 0.0) or 0.0
            specs.append({
                'phoneme': ph_name,
                'duration': duration,
                'overlap': overlap,
                'pitch_contour': pitch_points,
                'num_pitch_points': len(pitch_points),
                'f1': f1,
                'f2': f2,
                'f3': f3,
                'voiced': ph_name not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
            })
        except ValueError:
            continue
    return specs

def specs_to_readable(specs: List[Dict]) -> str:
    lines = ["# PHONEME  DUR    OVRLP  P0 [P1 P2 ...]"]
    for spec in specs:
        pitches = ' '.join(f"{p:.1f}" for p in spec['pitch_contour'])
        lines.append(f"{spec['phoneme']:4s} {spec['duration']:6.3f} {spec['overlap']:6.3f} {pitches}")
    return '\\n'.join(lines)

def save_parameterized_phonemes(filename: str, specs: List[Dict]):
    with open(filename, 'wb') as f:
        f.write(b'\\xDE\\xAD\\xBE\\xEF')
        for spec in specs:
            ph_id = PHONEME_TO_BYTE[spec['phoneme']]
            f.write(bytes([ph_id]))
            np.array([spec['duration']], dtype=np.float32).tofile(f)
            np.array([spec.get('overlap', 0.0)], dtype=np.float32).tofile(f)
            f.write(bytes([spec['num_pitch_points']]))
            pitches = spec['pitch_contour'] + [0.0] * (8 - spec['num_pitch_points'])
            np.array(pitches[:8], dtype=np.float32).tofile(f)
            np.array([spec['f1'], spec['f2'], spec['f3']], dtype=np.float32).tofile(f)

def load_parameterized_phonemes(filename: str) -> List[Dict]:
    with open(filename, 'rb') as f:
        magic = f.read(4)
        if magic != b'\\xDE\\xAD\\xBE\\xEF':
            raise ValueError(f"Not a valid PHX file (expected b'\\\\xDE\\\\xAD\\\\xBE\\\\xEF', got {magic})")
        specs = []
        while True:
            ph_byte = f.read(1)
            if not ph_byte:
                break
            ph_id = ph_byte[0]
            if ph_id not in BYTE_TO_PHONEME:
                raise ValueError(f"Invalid phoneme ID: 0x{ph_id:02X}")
            dur_arr = np.fromfile(f, dtype=np.float32, count=1)
            if len(dur_arr) < 1:
                break
            duration = float(dur_arr[0])
            overlap_arr = np.fromfile(f, dtype=np.float32, count=1)
            if len(overlap_arr) < 1:
                overlap = 0.0
            else:
                overlap = float(overlap_arr[0])
            num_pts_byte = f.read(1)
            if not num_pts_byte:
                break
            num_pts = num_pts_byte[0]
            pitches_arr = np.fromfile(f, dtype=np.float32, count=8)
            if len(pitches_arr) < 8:
                break
            formants_arr = np.fromfile(f, dtype=np.float32, count=3)
            if len(formants_arr) < 3:
                break
            specs.append({
                'phoneme': BYTE_TO_PHONEME[ph_id],
                'duration': duration,
                'overlap': max(0.0, min(0.5, overlap)),
                'pitch_contour': [float(p) for p in pitches_arr[:num_pts]],
                'num_pitch_points': num_pts,
                'f1': float(formants_arr[0]),
                'f2': float(formants_arr[1]),
                'f3': float(formants_arr[2]),
                'voiced': BYTE_TO_PHONEME[ph_id] not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
            })
    return specs

class FormantSynthesizer:
    def __init__(self, voice: Voice, sample_rate: int = smp):
        self.fs = sample_rate
        self.voice = voice
    
    def generate_glottal_pulse_train_contour(self, duration: float, pitch_contour: List[float]):
        n_samples = int(duration * self.fs)
        signal = np.zeros(n_samples)
        t = 0.0
        if not pitch_contour or all(p == 0 for p in pitch_contour):
            pitch_contour = [115.0]
        num_points = len(pitch_contour)
        while t < duration:
            t_norm = min(1.0, t / duration)
            if num_points == 1:
                f0 = pitch_contour[0]
            else:
                contour_pos = t_norm * (num_points - 1)
                idx_floor = int(contour_pos)
                frac = contour_pos - idx_floor
                if idx_floor >= num_points - 1:
                    f0 = pitch_contour[-1]
                else:
                    f0 = pitch_contour[idx_floor] * (1 - frac) + pitch_contour[idx_floor + 1] * frac
            f0 = max(50.0, min(400.0, f0))
            period_samples = self.fs / f0
            pulse_len = int(period_samples * 0.6)
            if pulse_len < 8:
                pulse_len = 8
            pulse = np.zeros(pulse_len)
            open_len = max(4, int(pulse_len * 0.4))
            pulse[:open_len] = -0.5 * (1 - np.cos(np.linspace(0, np.pi, open_len)))
            if pulse_len > open_len:
                close_len = pulse_len - open_len
                pulse[open_len:] = -0.1 * np.exp(-np.linspace(0, 5, close_len))
            start = int(t * self.fs)
            end = min(start + pulse_len, n_samples)
            if end > start:
                signal[start:end] += pulse[:end - start] * 0.6
            t += period_samples / self.fs
        peak = np.max(np.abs(signal))
        if peak > 0.1:
            signal = signal * (0.6 / peak)
        return signal
    
    def generate_shaped_noise(self, duration: float, phoneme: str, intensity: float = 0.25):
        n_samples = int(duration * self.fs)
        noise = np.random.randn(n_samples)
        if phoneme in {'S'}:
            b, a = sig.butter(6, [4000/(self.fs/2), 8500/(self.fs/2)], btype='band')
            noise = sig.filtfilt(b, a, noise)
            b2, a2 = sig.butter(4, 6500/(self.fs/2), btype='high')
            noise = sig.filtfilt(b2, a2, noise) * 1.3
        elif phoneme in {'SH', 'ZH'}:
            b, a = sig.butter(5, [2500/(self.fs/2), 6000/(self.fs/2)], btype='band')
            noise = sig.filtfilt(b, a, noise)
        elif phoneme in {'F', 'TH'}:
            b, a = sig.butter(4, 3500/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
        elif phoneme == 'HH':
            b, a = sig.butter(3, 2800/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
            noise += np.random.randn(n_samples) * 0.15
        elif phoneme in {'V', 'DH', 'Z'}:
            b, a = sig.butter(4, 4500/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
            voicing = np.sin(2 * np.pi * 120 * np.arange(n_samples) / self.fs) * 0.15
            noise = noise * 0.85 + voicing * 0.15
        else:
            b, a = sig.butter(4, 7500/(self.fs/2), btype='low')
            noise = sig.filtfilt(b, a, noise)
        peak = np.max(np.abs(noise))
        if peak < 1e-6:
            noise = np.random.randn(n_samples) * intensity * 0.7
            peak = 1.0
        noise = noise * (intensity / peak)
        return noise[:n_samples]
    
    def stable_resonator(self, freq: float, bw: float):
        if freq <= 0:
            return np.array([1.0]), np.array([1.0])
        w0 = 2 * np.pi * freq / self.fs
        bw_rad = max(2 * np.pi * bw / self.fs, 2 * np.pi * 80 / self.fs)
        a1 = -2 * np.exp(-bw_rad/2) * np.cos(w0)
        a2 = np.exp(-bw_rad)
        b0 = np.sqrt(1 - a2)
        return np.array([b0]), np.array([1.0, a1, a2])
    
    def apply_formants_safe(self, signal: np.ndarray, f1: float, f2: float, f3: float) -> np.ndarray:
        b1, b2, b3 = 60, 90, 150
        for freq, bw in [(f1, b1), (f2, b2), (f3, b3)]:
            if freq and freq > 50:
                b, a = self.stable_resonator(freq, bw)
                signal = sig.lfilter(b, a, signal)
        peak = np.max(np.abs(signal))
        if peak > 4.0:
            signal = signal * (3.0 / peak)
        b, a = sig.butter(1, 900/(self.fs/2), btype='high')
        return sig.lfilter(b, a, signal)
    
    def synthesize_phoneme_direct(self, spec: Dict) -> np.ndarray:
        ph = spec['phoneme']
        dur = spec['duration']
        f1, f2, f3 = spec['f1'], spec['f2'], spec['f3']
        pitch_contour = spec['pitch_contour']
        voiced = spec['voiced']
        
        if ph == 'SIL':
            return np.zeros(int(dur * self.fs))
        
        if ph in STOPS:
            n_samples = int(dur * self.fs)
            out = np.zeros(n_samples)
            closure_end = int(n_samples * 0.82)
            burst_start = closure_end
            burst_len = min(200, n_samples - burst_start)
            if burst_len > 30:
                burst_noise = np.random.randn(burst_len)
                if f1 > 50:
                    b1, a1 = self.stable_resonator(f1, 150)
                    b2, a2 = self.stable_resonator(f2, 200)
                    burst_noise = sig.lfilter(b1, a1, burst_noise)
                    burst_noise = sig.lfilter(b2, a2, burst_noise)
                burst_env = np.hanning(burst_len) * 0.6
                out[burst_start:burst_start+burst_len] = burst_noise * burst_env
            
            if ph in {'P', 'T', 'K', 'CH'} and closure_end + burst_len < n_samples:
                aspir_start = burst_start + burst_len
                aspir_len = n_samples - aspir_start
                if aspir_len > 50:
                    aspiration = self.generate_shaped_noise(aspir_len/self.fs, 'HH', intensity=0.18)
                    b, a = sig.butter(2, 800/(self.fs/2), btype='high')
                    aspiration = sig.filtfilt(b, a, aspiration)
                    out[aspir_start:] = aspiration[:aspir_len] * 0.4
            elif ph in {'B', 'D', 'G'} and closure_end + burst_len < n_samples:
                voicing_start = burst_start + int(burst_len * 1.3)
                voicing_len = n_samples - voicing_start
                if voicing_len > 100:
                    voicing = self.generate_glottal_pulse_train_contour(voicing_len/self.fs, [115.0])
                    out[voicing_start:] = voicing[:voicing_len] * 0.35
            return out * 0.85
        
        if not voiced:
            if ph == 'S':
                intensity = 0.64
            elif ph == 'SH':
                intensity = 0.32
            elif ph in {'F', 'TH'}:
                intensity = 0.32
            else:
                intensity = 0.25
            source = self.generate_shaped_noise(dur, ph, intensity=intensity)
            if f1 > 50:
                if ph in {'S', 'SH'}:
                    source = self.apply_formants_safe(source, f1*0.7, f2*0.7, f3*0.7)
                else:
                    source = self.apply_formants_safe(source, f1, f2, f3)
            else:
                source = source * 0.45
            output = source
        else:
            source = self.generate_glottal_pulse_train_contour(dur, pitch_contour)
            if f1 > 50:
                output = self.apply_formants_safe(source, f1, f2, f3)
            else:
                output = source * 0.45
        
        n = len(output)
        env = np.ones(n)
        att = min(0.007, dur * 0.12)
        rel = min(0.018, dur * 0.28)
        att_s = int(att * self.fs)
        rel_s = int(rel * self.fs)
        if att_s > 0:
            env[:att_s] = np.linspace(0, 1, att_s)
        if rel_s > 0:
            env[-rel_s:] = np.linspace(1, 0.05, rel_s)
        output = output * env
        output = np.tanh(output * 1.15) * 0.93
        return output * 0.82
    
    def synthesize_from_specs(self, specs: List[Dict]) -> np.ndarray:
        if not specs:
            return np.zeros(0)
        
        total_duration = 0.0
        for spec in specs:
            total_duration += spec['duration']
        
        for i in range(len(specs) - 1):
            total_duration -= min(specs[i].get('overlap', 0.0), specs[i]['duration'])
        
        total_samples = int(total_duration * self.fs) + 10
        output = np.zeros(total_samples)
        current_pos = 0
        
        for i, spec in enumerate(specs):
            phoneme_audio = self.synthesize_phoneme_direct(spec)
            phoneme_samples = len(phoneme_audio)
            
            overlap_dur = spec.get('overlap', 0.0)
            if i == len(specs) - 1:
                overlap_dur = 0.0
            
            overlap_samples = min(
                int(overlap_dur * self.fs),
                phoneme_samples - 1,
                int(spec['duration'] * self.fs * 0.5)
            )
            
            end_pos = current_pos + phoneme_samples
            if end_pos > len(output):
                output = np.resize(output, end_pos + 1000)
            
            output[current_pos:end_pos] += phoneme_audio
            
            current_pos += (phoneme_samples - overlap_samples)
        
        actual_length = min(current_pos, len(output))
        audio = output[:actual_length]
        
        audio = np.tanh(audio * 1.25) * 0.94
        b, a = sig.butter(5, 5000/(self.fs/2), btype='low')
        audio = sig.filtfilt(b, a, audio)
        return audio

def save_wav(filename: str, audio: np.ndarray, sr: int = smp):
    audio = np.clip(audio * 32767, -32768, 32767).astype(np.int16)
    with wave.open(filename, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sr)
        wf.writeframes(audio.tobytes())
    </div>
</body>
</html>
