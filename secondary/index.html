<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FSB4 Speech Synthesizer (Pyodide)</title>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js"></script>
    <style>
        body { font-family: Arial, sans-serif; max-width: 900px; margin: 20px auto; padding: 20px; background: #f5f7ff; }
        .container { background: white; border-radius: 10px; padding: 25px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }
        h1 { color: #5a67d8; text-align: center; margin-bottom: 5px; }
        .subtitle { text-align: center; color: #718096; margin-bottom: 25px; }
        .status { background: #2d3748; color: #68d391; padding: 12px; border-radius: 6px; font-family: monospace; margin: 15px 0; white-space: pre-wrap; }
        .editor { width: 100%; height: 200px; font-family: monospace; padding: 12px; border: 2px solid #cbd5e0; border-radius: 6px; margin: 15px 0; }
        button { background: #667eea; color: white; border: none; padding: 10px 20px; border-radius: 6px; cursor: pointer; font-weight: bold; margin: 5px; transition: all 0.2s; }
        button:hover { background: #5568d3; transform: translateY(-1px); }
        button:disabled { background: #a0aec0; cursor: not-allowed; transform: none; }
        .controls { display: flex; flex-wrap: wrap; gap: 10px; margin: 20px 0; justify-content: center; }
        .tabs { display: flex; margin-bottom: 20px; border-bottom: 2px solid #e2e8f0; }
        .tab { padding: 10px 20px; cursor: pointer; color: #718096; }
        .tab.active { color: #667eea; border-bottom: 3px solid #667eea; font-weight: bold; }
        .tab-content { display: none; }
        .tab-content.active { display: block; }
        .error { background: #fee2e2; color: #b91c1c; padding: 15px; border-radius: 6px; margin: 15px 0; }
        .success { background: #dcfce7; color: #15803d; padding: 15px; border-radius: 6px; margin: 15px 0; }
        .loading { text-align: center; padding: 30px; color: #667eea; }
        .spinner { border: 3px solid #cbd5e0; border-top: 3px solid #667eea; border-radius: 50%; width: 30px; height: 30px; animation: spin 1s linear infinite; margin: 0 auto 15px; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
        @media (max-width: 600px) {
            .controls { flex-direction: column; }
            button { width: 100%; }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>FSB4 Speech Synthesizer</h1>
        <div class="subtitle">Formant-based speech synthesis running entirely in your browser</div>
        
        <div id="loading-screen" class="loading">
            <div class="spinner"></div>
            <div id="loading-status">Loading Pyodide runtime...</div>
        </div>
        
        <div id="main-app" style="display:none;">
            <div class="tabs">
                <div class="tab active" data-tab="editor">Editor</div>
                <div class="tab" data-tab="reference">Reference</div>
            </div>
            
            <div class="tab-content active" id="editor-tab">
                <div class="status" id="status-bar">Status: Ready to load engine...</div>
                
                <label>Phoneme Spec (PHONEME DUR OVRLP P0 [P1...]):</label>
                <textarea class="editor" id="spec-editor" spellcheck="false">SIL  0.190 0.000 0.0
HH   0.080 0.012 115.0
EH   0.120 0.025 115.0
L    0.110 0.015 115.0
OW   0.180 0.000 105.0
SIL  0.280 0.000 0.0</textarea>
                
                <div class="controls">
                    <button onclick="parseSpec()">Parse to Phonemes</button>
                    <button onclick="renderAudio()">Render Audio</button>
                    <button id="play-btn" onclick="playAudio()" disabled>â–¶ Play</button>
                    <button onclick="exportWAV()" disabled>ðŸ’¾ Export WAV</button>
                </div>
                
                <div id="error-container"></div>
            </div>
            
            <div class="tab-content" id="reference-tab">
                <div class="status">
PHONEME FORMAT (OVRLP):
PHONEME DUR(sec) OVRLP(sec) P0(Hz) [P1...]

VOWELS: AH AE AA AO EH EY IH IY OW UH UW ER
CONSONANTS: B D G P T K M N NG L R F S SH TH V Z ZH DH W Y HH CH JH

EXAMPLE:
SIL  0.190 0.000 0.0
HH   0.080 0.012 115.0
EH   0.120 0.025 115.0
L    0.110 0.015 115.0
OW   0.180 0.000 105.0
SIL  0.280 0.000 0.0

â†’ OVRLP enables smooth crossfading between phonemes
â†’ Typical values: vowels 0.018-0.030s, consonants 0.008-0.015s
                </div>
            </div>
        </div>
    </div>

    <script>
        let pyodide = null;
        let renderedAudio = null;
        let audioContext = null;
        let isPlaying = false;
        
        // Initialize Pyodide and load FSB4
        async function loadEngine() {
            try {
                updateStatus("Loading Pyodide runtime (may take 15-30s on first load)...");
                pyodide = await loadPyodide();
                
                updateStatus("Installing numpy and scipy...");
                await pyodide.loadPackage(["numpy", "scipy"]);
                
                updateStatus("Initializing FSB4 engine...");
                await pyodide.runPythonAsync(`
import sys
import numpy as np
import scipy.signal as sig
import wave
import re
from typing import Dict, List

# Minimal FSB4 implementation for browser
smp = 48000

BYTE_TO_PHONEME = {
    0x00: 'SIL', 0x01: 'AH', 0x02: 'AE', 0x03: 'AA', 0x04: 'AO', 0x05: 'EH', 0x06: 'EY',
    0x07: 'IH', 0x08: 'IY', 0x09: 'OW', 0x0A: 'UH', 0x0B: 'UW', 0x0C: 'ER', 0x0D: 'B',
    0x0E: 'D', 0x0F: 'G', 0x10: 'P', 0x11: 'T', 0x12: 'K', 0x13: 'M', 0x14: 'N', 0x15: 'NG',
    0x16: 'L', 0x17: 'R', 0x18: 'F', 0x19: 'S', 0x1A: 'SH', 0x1B: 'TH', 0x1C: 'DH', 0x1D: 'V',
    0x1E: 'Z', 0x1F: 'ZH', 0x20: 'W', 0x21: 'Y', 0x22: 'HH', 0x23: 'CH', 0x24: 'JH',
}
PHONEME_TO_BYTE = {v: k for k, v in BYTE_TO_PHONEME.items()}

VOWELS = {'AH','AE','AA','AO','EH','EY','IH','IY','OW','UH','UW','ER'}
STOPS = {'P','T','K','B','D','G','CH'}

class DefaultVoice:
    def __init__(self):
        self.phonemes = {
            'AH': {'f1': 700, 'f2': 1100, 'f3': 2400, 'length': 0.14, 'voiced': True},
            'AE': {'f1': 650, 'f2': 1250, 'f3': 2500, 'length': 0.14, 'voiced': True},
            'AA': {'f1': 620, 'f2': 1180, 'f3': 2550, 'length': 0.14, 'voiced': True},
            'AO': {'f1': 550, 'f2': 850, 'f3': 2400, 'length': 0.14, 'voiced': True},
            'EH': {'f1': 530, 'f2': 1700, 'f3': 2450, 'length': 0.14, 'voiced': True},
            'EY': {'f1': 400, 'f2': 2100, 'f3': 2800, 'length': 0.14, 'voiced': True},
            'IH': {'f1': 420, 'f2': 1950, 'f3': 2500, 'length': 0.14, 'voiced': True},
            'IY': {'f1': 300, 'f2': 2250, 'f3': 3000, 'length': 0.14, 'voiced': True},
            'OW': {'f1': 450, 'f2': 900, 'f3': 2350, 'length': 0.14, 'voiced': True},
            'UH': {'f1': 400, 'f2': 650, 'f3': 2400, 'length': 0.14, 'voiced': True},
            'UW': {'f1': 330, 'f2': 900, 'f3': 2200, 'length': 0.14, 'voiced': True},
            'ER': {'f1': 480, 'f2': 1180, 'f3': 1650, 'length': 0.14, 'voiced': True},
            'M':  {'f1': 350, 'f2': 1050, 'f3': 2250, 'length': 0.12, 'voiced': True},
            'N':  {'f1': 320, 'f2': 1150, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'NG': {'f1': 280, 'f2': 950, 'f3': 2350, 'length': 0.12, 'voiced': True},
            'L':  {'f1': 400, 'f2': 1150, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'R':  {'f1': 450, 'f2': 1250, 'f3': 1500, 'length': 0.12, 'voiced': True},
            'DH': {'f1': 380, 'f2': 1650, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'V':  {'f1': 380, 'f2': 1550, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'Z':  {'f1': 380, 'f2': 1750, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'ZH': {'f1': 380, 'f2': 1450, 'f3': 2250, 'length': 0.12, 'voiced': True},
            'W':  {'f1': 350, 'f2': 700, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'Y':  {'f1': 350, 'f2': 2050, 'f3': 2650, 'length': 0.12, 'voiced': True},
            'JH': {'f1': 400, 'f2': 1650, 'f3': 2450, 'length': 0.12, 'voiced': True},
            'B':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'D':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'G':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'P':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'T':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'K':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'F':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.125, 'voiced': False},
            'S':  {'f1': None, 'f2': None, 'f3': None, 'length': 0.125, 'voiced': False},
            'SH': {'f1': None, 'f2': None, 'f3': None, 'length': 0.125, 'voiced': False},
            'TH': {'f1': None, 'f2': None, 'f3': None, 'length': 0.125, 'voiced': False},
            'HH': {'f1': None, 'f2': None, 'f3': None, 'length': 0.125, 'voiced': False},
            'CH': {'f1': None, 'f2': None, 'f3': None, 'length': 0.068, 'voiced': False},
            'SIL': {'f1': 0, 'f2': 0, 'f3': 0, 'length': 0.19, 'voiced': 'silence'},
        }
    
    def get_phoneme_data(self, phoneme):
        if phoneme.endswith('_FINAL'):
            base = phoneme.replace('_FINAL', '')
            data = self.phonemes.get(base, self.phonemes.get('SIL', {})).copy()
            if base in VOWELS:
                data['length'] = min(data.get('length', 0.14) * 1.4, 0.35)
            return data
        return self.phonemes.get(phoneme, self.phonemes.get('SIL', {}))

def parse_phoneme_spec(text, voice):
    specs = []
    for line in text.splitlines():
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        parts = line.split()
        if len(parts) < 4:
            continue
        ph = parts[0].upper()
        if ph not in PHONEME_TO_BYTE:
            continue
        try:
            dur = max(0.01, min(2.0, float(parts[1])))
            overlap = max(0.0, min(0.5, float(parts[2])))
            pitches = [float(p) for p in parts[3:]]
            if len(pitches) > 8:
                pitches = pitches[:8]
            ph_data = voice.get_phoneme_data(ph)
            f1 = ph_data.get('f1', 0.0) or 0.0
            f2 = ph_data.get('f2', 0.0) or 0.0
            f3 = ph_data.get('f3', 0.0) or 0.0
            specs.append({
                'phoneme': ph,
                'duration': dur,
                'overlap': overlap,
                'pitch_contour': pitches,
                'num_pitch_points': len(pitches),
                'f1': f1,
                'f2': f2,
                'f3': f3,
                'voiced': ph not in {'SIL','B','D','G','P','T','K','F','S','SH','TH','HH','CH'}
            })
        except:
            continue
    return specs

def stable_resonator(fs, freq, bw):
    if freq <= 0:
        return np.array([1.0]), np.array([1.0])
    w0 = 2 * np.pi * freq / fs
    bw_rad = max(2 * np.pi * bw / fs, 2 * np.pi * 80 / fs)
    a1 = -2 * np.exp(-bw_rad/2) * np.cos(w0)
    a2 = np.exp(-bw_rad)
    b0 = np.sqrt(1 - a2)
    return np.array([b0]), np.array([1.0, a1, a2])

def apply_formants(fs, signal, f1, f2, f3):
    for freq, bw in [(f1, 60), (f2, 90), (f3, 150)]:
        if freq and freq > 50:
            b, a = stable_resonator(fs, freq, bw)
            signal = sig.lfilter(b, a, signal)
    b, a = sig.butter(1, 900/(fs/2), btype='high')
    return sig.lfilter(b, a, signal)

def synthesize_phoneme(fs, spec):
    ph = spec['phoneme']
    dur = spec['duration']
    f1, f2, f3 = spec['f1'], spec['f2'], spec['f3']
    pitch = spec['pitch_contour'][0] if spec['pitch_contour'] else 115.0
    voiced = spec['voiced']
    
    n_samples = int(dur * fs)
    if ph == 'SIL':
        return np.zeros(n_samples)
    
    if ph in STOPS:
        out = np.zeros(n_samples)
        closure_end = int(n_samples * 0.82)
        burst_start = closure_end
        burst_len = min(200, n_samples - burst_start)
        if burst_len > 30:
            burst = np.random.randn(burst_len)
            if f1 > 50:
                b1, a1 = stable_resonator(fs, f1, 150)
                b2, a2 = stable_resonator(fs, f2, 200)
                burst = sig.lfilter(b1, a1, burst)
                burst = sig.lfilter(b2, a2, burst)
            env = np.hanning(burst_len) * 0.6
            out[burst_start:burst_start+burst_len] = burst * env
        return out * 0.85
    
    if not voiced:
        noise = np.random.randn(n_samples)
        b, a = sig.butter(4, 7500/(fs/2), btype='low')
        noise = sig.filtfilt(b, a, noise)
        if f1 > 50:
            noise = apply_formants(fs, noise, f1, f2, f3)
        output = noise * 0.3
    else:
        t = np.arange(n_samples) / fs
        glottal = np.sin(2 * np.pi * pitch * t)
        if f1 > 50:
            output = apply_formants(fs, glottal, f1, f2, f3)
        else:
            output = glottal * 0.45
    
    env = np.ones(n_samples)
    att = min(0.007, dur * 0.12)
    rel = min(0.018, dur * 0.28)
    att_s = int(att * fs)
    rel_s = int(rel * fs)
    if att_s > 0:
        env[:att_s] = np.linspace(0, 1, att_s)
    if rel_s > 0:
        env[-rel_s:] = np.linspace(1, 0.05, rel_s)
    output = output * env
    output = np.tanh(output * 1.15) * 0.93
    return output * 0.82

def synthesize_from_specs(fs, specs):
    if not specs:
        return np.zeros(0)
    
    total_dur = sum(s['duration'] for s in specs)
    for i in range(len(specs) - 1):
        total_dur -= min(specs[i].get('overlap', 0.0), specs[i]['duration'])
    
    total_samples = int(total_dur * fs) + 10
    output = np.zeros(total_samples)
    pos = 0
    
    for i, spec in enumerate(specs):
        audio = synthesize_phoneme(fs, spec)
        samples = len(audio)
        
        overlap = spec.get('overlap', 0.0) if i < len(specs) - 1 else 0.0
        overlap_samples = min(int(overlap * fs), samples - 1, int(spec['duration'] * fs * 0.5))
        
        end = pos + samples
        if end > len(output):
            output = np.resize(output, end + 1000)
        
        output[pos:end] += audio
        pos += (samples - overlap_samples)
    
    actual_len = min(pos, len(output))
    audio = output[:actual_len]
    audio = np.tanh(audio * 1.25) * 0.94
    b, a = sig.butter(5, 5000/(fs/2), btype='low')
    audio = sig.filtfilt(b, a, audio)
    return audio

# Export minimal API
import js
js.window.fsb_parse = parse_phoneme_spec
js.window.fsb_synthesize = synthesize_from_specs
js.window.fsb_voice = DefaultVoice()
js.window.fsb_smp = smp
                `);
                
                document.getElementById('loading-screen').style.display = 'none';
                document.getElementById('main-app').style.display = 'block';
                updateStatus("âœ“ FSB4 engine loaded successfully! Parse spec to begin.");
                setupTabs();
            } catch (error) {
                console.error("Pyodide initialization failed:", error);
                document.getElementById('loading-status').textContent = 
                    `FAILED: ${error.message || error.toString()}\n\nCheck console for details (F12)`;
                document.getElementById('loading-status').style.color = '#e53e3e';
                document.querySelector('.spinner').style.borderColor = '#e53e3e';
                document.querySelector('.spinner').style.borderTopColor = '#e53e3e';
            }
        }
        
        function updateStatus(msg) {
            document.getElementById('status-bar').textContent = `Status: ${msg}`;
        }
        
        function setupTabs() {
            document.querySelectorAll('.tab').forEach(tab => {
                tab.addEventListener('click', () => {
                    document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
                    tab.classList.add('active');
                    document.getElementById(tab.dataset.tab + '-tab').classList.add('active');
                });
            });
        }
        
        async function parseSpec() {
            if (!pyodide) {
                updateStatus("Engine not loaded yet - please wait");
                return;
            }
            
            try {
                const text = document.getElementById('spec-editor').value.trim();
                if (!text) {
                    updateStatus("ERROR: No spec text!");
                    return;
                }
                
                updateStatus("Parsing phoneme spec...");
                const specs = await pyodide.globals.get('fsb_parse')(text, pyodide.globals.get('fsb_voice'));
                const specsJs = specs.toJs();
                
                if (!specsJs || specsJs.length < 2) {
                    updateStatus("ERROR: No valid phonemes parsed!");
                    return;
                }
                
                // Show normalized format
                let output = "# PHONEME  DUR    OVRLP  P0 [P1...]\n";
                specsJs.forEach(spec => {
                    const pitches = spec.pitch_contour.map(p => p.toFixed(1)).join(' ');
                    output += `${spec.phoneme.padEnd(4)} ${spec.duration.toFixed(3)} ${spec.overlap.toFixed(3)} ${pitches}\n`;
                });
                document.getElementById('spec-editor').value = output.trim();
                
                const totalDur = specsJs.reduce((sum, s) => sum + s.duration, 0);
                updateStatus(`âœ“ Parsed ${specsJs.length} phonemes (${totalDur.toFixed(2)}s with overlap). Click "Render Audio" to synthesize.`);
            } catch (error) {
                console.error("Parse error:", error);
                showError(`Parse failed: ${error.message || error.toString()}`);
                updateStatus("ERROR during parsing - see details above");
            }
        }
        
        async function renderAudio() {
            if (!pyodide) {
                updateStatus("Engine not loaded yet");
                return;
            }
            
            try {
                updateStatus("Parsing spec...");
                const text = document.getElementById('spec-editor').value.trim();
                const specs = await pyodide.globals.get('fsb_parse')(text, pyodide.globals.get('fsb_voice'));
                const specsJs = specs.toJs();
                
                if (!specsJs || specsJs.length < 2) {
                    updateStatus("ERROR: No valid phonemes to render!");
                    return;
                }
                
                updateStatus("Synthesizing audio with crossfade blending...");
                const audioBuffer = await pyodide.globals.get('fsb_synthesize')(
                    pyodide.globals.get('fsb_smp'),
                    specs
                );
                
                renderedAudio = audioBuffer.toJs();
                const duration = renderedAudio.length / 48000;
                
                // Enable play button
                document.getElementById('play-btn').disabled = false;
                document.querySelector('button[onclick="exportWAV()"]').disabled = false;
                
                updateStatus(`âœ“ Audio rendered (${duration.toFixed(2)}s with crossfade). Click PLAY to hear.`);
            } catch (error) {
                console.error("Render error:", error);
                showError(`Render failed: ${error.message || error.toString()}`);
                updateStatus("ERROR during rendering - see details above");
            }
        }
        
        function playAudio() {
            if (!renderedAudio || renderedAudio.length === 0) {
                updateStatus("No audio to play - render first!");
                return;
            }
            
            if (isPlaying) {
                stopAudio();
                return;
            }
            
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }
                
                const buffer = audioContext.createBuffer(1, renderedAudio.length, 48000);
                const channelData = buffer.getChannelData(0);
                for (let i = 0; i < renderedAudio.length; i++) {
                    channelData[i] = renderedAudio[i];
                }
                
                const source = audioContext.createBufferSource();
                source.buffer = buffer;
                source.connect(audioContext.destination);
                source.start(0);
                
                isPlaying = true;
                document.getElementById('play-btn').textContent = 'â–  Stop';
                updateStatus("Playing audio with crossfade blending...");
                
                source.onended = () => {
                    isPlaying = false;
                    document.getElementById('play-btn').textContent = 'â–¶ Play';
                    updateStatus("Playback complete");
                };
            } catch (error) {
                console.error("Playback error:", error);
                updateStatus(`Playback error: ${error.message}`);
            }
        }
        
        function stopAudio() {
            if (audioContext) {
                audioContext.close().then(() => {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                });
            }
            isPlaying = false;
            document.getElementById('play-btn').textContent = 'â–¶ Play';
            updateStatus("Playback stopped");
        }
        
        function exportWAV() {
            if (!renderedAudio || renderedAudio.length === 0) {
                updateStatus("No audio to export - render first!");
                return;
            }
            
            try {
                // Create WAV file manually (no scipy/wave needed)
                const sampleRate = 48000;
                const numSamples = renderedAudio.length;
                const buffer = new ArrayBuffer(44 + numSamples * 2);
                const view = new DataView(buffer);
                
                // WAV header
                writeString(view, 0, 'RIFF');
                view.setUint32(4, 36 + numSamples * 2, true);
                writeString(view, 8, 'WAVE');
                writeString(view, 12, 'fmt ');
                view.setUint32(16, 16, true);
                view.setUint16(20, 1, true);
                view.setUint16(22, 1, true);
                view.setUint32(24, sampleRate, true);
                view.setUint32(28, sampleRate * 2, true);
                view.setUint16(32, 2, true);
                view.setUint16(34, 16, true);
                writeString(view, 36, 'data');
                view.setUint32(40, numSamples * 2, true);
                
                // PCM data
                const int16Array = new Int16Array(buffer, 44);
                for (let i = 0; i < numSamples; i++) {
                    const s = Math.max(-1, Math.min(1, renderedAudio[i]));
                    int16Array[i] = s < 0 ? s * 32768 : s * 32767;
                }
                
                // Download
                const blob = new Blob([buffer], { type: 'audio/wav' });
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = 'fsb4_output.wav';
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
                
                const sizeKB = buffer.byteLength / 1024;
                const duration = numSamples / sampleRate;
                updateStatus(`âœ“ WAV exported (${sizeKB.toFixed(1)} KB, ${duration.toFixed(2)}s)`);
            } catch (error) {
                console.error("Export error:", error);
                updateStatus(`Export failed: ${error.message}`);
            }
        }
        
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }
        
        function showError(message) {
            const container = document.getElementById('error-container');
            container.innerHTML = `<div class="error">Error: ${message.replace(/\n/g, '<br>')}</div>`;
            setTimeout(() => {
                container.innerHTML = '';
            }, 10000);
        }
        
        // Start loading engine immediately
        loadEngine();
    </script>
</body>
</html>
